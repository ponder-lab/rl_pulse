{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design Using Reinforcement Learning\n",
    "\n",
    "{{ explanation for my approach, what I'm trying to do }}\n",
    "\n",
    "Currently looking to implement DDPG for RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [OpenAI SpinningUp resource](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode) for the theoretical background on DDPG, and lots of TensorFlow documentation for how to write the algorithm below.\n",
    "\n",
    "For the policy function, I need to perform gradient ascent with the following gradient\n",
    "$$\n",
    "\\nabla_\\theta 1/|B| \\sum_{s \\in B} Q_\\phi (s, \\pi_\\theta(s))\n",
    "$$\n",
    "\n",
    "And for the Q-function, perform gradient descent with\n",
    "$$\n",
    "\\nabla_\\phi 1/|B| \\sum_{(s,a,r,s',d) \\in B} (Q_\\phi(s,a) - y(r,s',d))^2\n",
    "$$\n",
    "\n",
    "Other resources:\n",
    "\n",
    "- https://keras.io/getting-started/sequential-model-guide/\n",
    "- https://www.tensorflow.org/guide/keras/overview\n",
    "- https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function\n",
    "- https://github.com/floodsung/DDPG/blob/master/actor_network.py\n",
    "- https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG.py\n",
    "\n",
    "Also helpful: https://www.tensorflow.org/guide/migrate#customize_the_training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinSimulation as ss\n",
    "import rlPulse as rlp\n",
    "import numpy as np\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ss)\n",
    "importlib.reload(rlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define system parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "dim = 2**N\n",
    "\n",
    "pulse = .25e-6    # duration of pulse\n",
    "delay = 3e-6      # duration of delay\n",
    "f1 = 1/(4*pulse)  # for pi/2 pulses\n",
    "coupling = 5e3    # coupling strength\n",
    "Delta = 500       # chemical shift strength (for identical spins)\n",
    "\n",
    "a = ss.getRandDip(N) # random dipolar coupling strengths\n",
    "(x,y,z) = (ss.x, ss.y, ss.z)\n",
    "(X,Y,Z) = ss.getCollectiveObservables(N, dim)\n",
    "\n",
    "Hdip = ss.getHdip(N, dim, x, y, z, a)\n",
    "Hint = ss.getHint(Hdip, coupling, Z, Delta)\n",
    "HWHH0 = ss.getHWHH0(X,Y,Z,Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the actor and critic, as well as target actor and target critic. The actor learns the policy function\n",
    "$$\n",
    "\\pi_\\theta: S \\to A, s \\mapsto a\n",
    "$$\n",
    "that picks the optimal action $a$ for a given state $s$, with some set of parameters $\\theta$. The critic learns the Q-function\n",
    "$$\n",
    "Q_\\phi: S \\times A \\to \\mathbf{R}, (s,a) \\mapsto q\n",
    "$$\n",
    "where $q$ is the total expected rewards by doing action $a$ on a state $s$, and $\\phi$ is the parameter set for the Q-function model. The target actor/critic have different parameter sets $\\theta_\\text{target}$ and $\\phi_\\text{target}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = rlp.Actor(3,3,None)\n",
    "actorTarget = rlp.Actor(3,3,None)\n",
    "critic = rlp.Critic(3,3,None,.5)\n",
    "criticTarget = rlp.Critic(3,3,None,.5)\n",
    "\n",
    "replayBuffer = rlp.ReplayBuffer(100) # TODO figure out if this buffer size makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implementation of the DDPG algorithm (see [this OpenAI resource for reference](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(10), np.random.normal(size=(10)), label='0')\n",
    "plt.title('Random numbers')\n",
    "plt.xlabel('Cycle number')\n",
    "plt.ylabel('Net magnetization, real')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
