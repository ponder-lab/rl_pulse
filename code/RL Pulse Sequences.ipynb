{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design Using Reinforcement Learning\n",
    "\n",
    "Implementing deep deterministic policy gradient (DDPG) to learn pulse sequence design for spin systems.\n",
    "\n",
    "TODO write up a more thorough background on the DDPG algorithm\n",
    "\n",
    "For training, the following reward function was used\n",
    "$$\n",
    "r = -\\log\\left( 1- \\left| \\text{Tr}\\left( \\frac{U_\\text{target}^\\dagger U_\\text{exp}}{D} \\right) \\right| \\right)\n",
    "= -\\log\\left( 1- \\text{fidelity}(U_\\text{target}^\\dagger, U_\\text{exp}) \\right)\n",
    "$$\n",
    "For example, if the fidelity is $0.999$, then the reward $r = -\\log(0.001) = 3$. \n",
    "\n",
    "Using the [OpenAI SpinningUp resource](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode) for the theoretical background on DDPG, and lots of TensorFlow documentation for how to write the algorithm below.\n",
    "\n",
    "<!-- For the policy function, I need to perform gradient ascent with the following gradient\n",
    "$$\n",
    "\\nabla_\\theta 1/|B| \\sum_{s \\in B} Q_\\phi (s, \\pi_\\theta(s))\n",
    "$$\n",
    "\n",
    "And for the Q-function, perform gradient descent with\n",
    "$$\n",
    "\\nabla_\\phi 1/|B| \\sum_{(s,a,r,s',d) \\in B} (Q_\\phi(s,a) - y(r,s',d))^2\n",
    "$$ -->\n",
    "\n",
    "Other resources:\n",
    "\n",
    "- https://keras.io/getting-started/sequential-model-guide/\n",
    "- https://www.tensorflow.org/guide/keras/overview\n",
    "- https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function\n",
    "- https://github.com/floodsung/DDPG/blob/master/actor_network.py\n",
    "- https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG.py\n",
    "\n",
    "Also helpful: https://www.tensorflow.org/guide/migrate#customize_the_training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinSimulation as ss\n",
    "import rlPulse as rlp\n",
    "import numpy as np\n",
    "import scipy.linalg as spla\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ss)\n",
    "importlib.reload(rlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize spin system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "dim = 2**N\n",
    "\n",
    "pulse = .25e-6    # duration of pulse\n",
    "delay = 3e-6      # duration of delay\n",
    "f1 = 1/(4*pulse)  # for pi/2 pulses\n",
    "coupling = 5e3    # coupling strength\n",
    "delta = 500       # chemical shift strength (for identical spins)\n",
    "\n",
    "(x,y,z) = (ss.x, ss.y, ss.z)\n",
    "(X,Y,Z) = ss.getTotalSpin(N, dim)\n",
    "\n",
    "Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "HWHH0 = ss.getHWHH0(X,Y,Z,delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize RL algorithm\n",
    "\n",
    "The target network parameters $\\theta_\\text{target}$ are updated by\n",
    "$$\n",
    "\\theta_\\text{target} = \\rho \\theta_\\text{target} + (1-\\rho)\\theta\n",
    "$$\n",
    "\n",
    "TODO figure out if this buffer size makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDim = 3 # state represented by sequences of actions...\n",
    "aDim = 3 # action = [phi, rot, time]\n",
    "\n",
    "numExp = 2000 # how many experiences to \"play\" through and learn from\n",
    "bufferSize = 500 # size of the replay buffer (i.e. how many experiences to keep in memory).\n",
    "batchSize = 50 # size of batch (subset of replay buffer) to use as training for actor and critic.\n",
    "p = 1 # action noise parameter\n",
    "polyak = 0.75 # polyak averaging parameter\n",
    "gamma = 0.5 # future reward discount rate\n",
    "\n",
    "printEvery = 50\n",
    "updateAfter = 500 # start updating actor/critic networks after this many episodes\n",
    "updateEvery = 50  # update networks every __ episodes\n",
    "numUpdates = 2 # how many training updates to perform on a random subset of experiences (s,a,r,s1,d)\n",
    "randomizeDipolarEvery = 10\n",
    "lowerNoiseAfter = 500\n",
    "\n",
    "\n",
    "pDiff = (0-p)/(numExp-lowerNoiseAfter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the actor and critic, as well as target actor and target critic. The actor learns the policy function\n",
    "$$\n",
    "\\pi_\\theta: S \\to A, s \\mapsto a\n",
    "$$\n",
    "that picks the optimal action $a$ for a given state $s$, with some set of parameters $\\theta$ (in this case weights/biases in the neural network). The critic learns the Q-function\n",
    "$$\n",
    "Q_\\phi: S \\times A \\to \\mathbf{R}, (s,a) \\mapsto q\n",
    "$$\n",
    "where $q$ is the total expected rewards by doing action $a$ on a state $s$, and $\\phi$ is the parameter set for the Q-function model. The target actor/critic have different parameter sets $\\theta_\\text{target}$ and $\\phi_\\text{target}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = rlp.Actor(sDim,aDim,None)\n",
    "actorTarget = rlp.Actor(sDim,aDim,None)\n",
    "critic = rlp.Critic(sDim,aDim,None, gamma)\n",
    "criticTarget = rlp.Critic(sDim,aDim,None, gamma)\n",
    "env = rlp.Environment(N, dim, sDim, HWHH0, X, Y)\n",
    "\n",
    "actorTarget.setParams(actor.getParams())\n",
    "criticTarget.setParams(critic.getParams())\n",
    "\n",
    "replayBuffer = rlp.ReplayBuffer(bufferSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rMat = np.zeros((numExp,))\n",
    "aMat = np.zeros((numExp,aDim))\n",
    "timeMat = np.zeros((numExp, 2)) # record length of episode so far and number of pulses\n",
    "# keep track of when resets/updates happen\n",
    "resetStateEps = []\n",
    "updateEps = []\n",
    "\n",
    "for i in tqdm(range(numExp)):\n",
    "    # randomize dipolar coupling strengths for Hint\n",
    "    if i > 0 and i % randomizeDipolarEvery == 0:\n",
    "        Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "    \n",
    "    s = env.getState()\n",
    "    # get action based on current state and some level of noise\n",
    "    a = rlp.clipAction(actor.predict(env.state) + rlp.actionNoise(p))\n",
    "    # evolve state based on action\n",
    "    env.evolve(a, Hint)\n",
    "    # get reward\n",
    "    r = env.reward()\n",
    "    \n",
    "    # record episode data\n",
    "    aMat[i,:] = a\n",
    "    rMat[i] = r\n",
    "    timeMat[i,:] = [env.t, np.sum(env.state[:,2] != 0)]\n",
    "    \n",
    "    # get updated state, and whether it's a terminal state\n",
    "    s1 = env.getState()\n",
    "    d = env.isDone()\n",
    "    replayBuffer.add(s,a,r,s1,d)\n",
    "    \n",
    "    # update noise parameter\n",
    "    if i > lowerNoiseAfter:\n",
    "        p += pDiff\n",
    "    \n",
    "    # CHECK IF TERMINAL\n",
    "    if d:\n",
    "        env.reset()\n",
    "        resetStateEps.append(i)\n",
    "    # UPDATE NETWORKS\n",
    "    if (i > updateAfter) and (i % updateEvery == 0):\n",
    "#         print(\"updating actor/critic networks (episode {})\".format(i))\n",
    "        updateEps.append(i)\n",
    "        for update in range(numUpdates):\n",
    "            batch = replayBuffer.getSampleBatch(batchSize)\n",
    "            # train critic\n",
    "            critic.trainStep(batch, actorTarget, criticTarget)\n",
    "            # train actor\n",
    "            actor.trainStep(batch, critic)\n",
    "            # update target networks\n",
    "            criticTarget.updateParams(critic, polyak)\n",
    "            actorTarget.updateParams(actor, polyak)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "See how the rewards and actions change over time as the actor/critic (hopefully) learn how to control the spin system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rMat, color='black', label='rewards')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.title('Rewards for each episode')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist(rMat, bins=20, color='black', label='rewards')\n",
    "# ymin, ymax = plt.ylim()\n",
    "# plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.title('Rewards histogram')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,0], 'ok', label='phi')\n",
    "plt.title('Phi action')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Phi action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,1], 'ok', label='rot')\n",
    "plt.title('Rot action')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Rot action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,2], 'ok', label='time')\n",
    "plt.title('Time action')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Time action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the state with highest reward in the buffer (might not be highest of all episodes because buffer forgets)\n",
    "rBuffer = [_[2] for _  in replayBuffer.buffer]\n",
    "rlp.printAction(replayBuffer.buffer[np.argmax(rBuffer)][3])\n",
    "print(\"reward: \", np.max(rBuffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate average fidelity for a pulse sequence\n",
    "\n",
    "fidelities = np.zeros((50,))\n",
    "for i in range(50):\n",
    "    Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "    Uexp = spla.expm(-1j*(Hint*10.0e-6 - X*0*np.pi)) @ \\\n",
    "           spla.expm(-1j*(Hint*3.47e-6 - X*0*np.pi)) @ \\\n",
    "           spla.expm(-1j*(Hint*7.11e-6 + Y*0*np.pi))\n",
    "    Utarget = ss.getPropagator(HWHH0, (7.11+3.47+10)*1e-6)\n",
    "    fidelities[i] = ss.fidelity(np.power(Utarget,1), np.power(Uexp,1))\n",
    "print(\"mean fidelity: \", np.mean(fidelities), \"std dev: \", np.std(fidelities), \"max: \", np.max(fidelities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
