{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design Using Reinforcement Learning\n",
    "\n",
    "{{ explanation for my approach, what I'm trying to do }}\n",
    "\n",
    "Currently looking to implement DDPG for RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [OpenAI SpinningUp resource](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode) for the theoretical background on DDPG, and lots of TensorFlow documentation for how to write the algorithm below.\n",
    "\n",
    "For the policy function, I need to perform gradient ascent with the following gradient\n",
    "$$\n",
    "\\nabla_\\theta 1/|B| \\sum_{s \\in B} Q_\\phi (s, \\pi_\\theta(s))\n",
    "$$\n",
    "\n",
    "And for the Q-function, perform gradient descent with\n",
    "$$\n",
    "\\nabla_\\phi 1/|B| \\sum_{(s,a,r,s',d) \\in B} (Q_\\phi(s,a) - y(r,s',d))^2\n",
    "$$\n",
    "\n",
    "Other resources:\n",
    "\n",
    "- https://keras.io/getting-started/sequential-model-guide/\n",
    "- https://www.tensorflow.org/guide/keras/overview\n",
    "- https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function\n",
    "- https://github.com/floodsung/DDPG/blob/master/actor_network.py\n",
    "- https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG.py\n",
    "\n",
    "Also helpful: https://www.tensorflow.org/guide/migrate#customize_the_training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinSimulation as ss\n",
    "import rlPulse as rlp\n",
    "import numpy as np\n",
    "import scipy.linalg as spla\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ss)\n",
    "importlib.reload(rlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define system parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "dim = 2**N\n",
    "\n",
    "pulse = .25e-6    # duration of pulse\n",
    "delay = 3e-6      # duration of delay\n",
    "f1 = 1/(4*pulse)  # for pi/2 pulses\n",
    "coupling = 5e3    # coupling strength\n",
    "delta = 500       # chemical shift strength (for identical spins)\n",
    "\n",
    "(x,y,z) = (ss.x, ss.y, ss.z)\n",
    "(X,Y,Z) = ss.getTotalSpin(N, dim)\n",
    "\n",
    "Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "HWHH0 = ss.getHWHH0(X,Y,Z,delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the RL algorithm parameters.\n",
    "\n",
    "- `numExp`: Specifies how many experiences to \"play\" through.\n",
    "- `numUpdates`: How many updates to perform using a random subset of experiences from the replay buffer.\n",
    "- `bufferSize`: Size of the replay buffer (i.e. how many experiences to keep in memory).\n",
    "- `batchSize`: Size of batch (subset of replay buffer) to use as training for actor and critic.\n",
    "- `p`: Action noise level (determines probabilities of rotating along a different axis or by different angle)\n",
    "- `polyak`: Polyak averaging parameter. The target network parameters $\\theta_\\text{target}$ are updated by\n",
    "$$\n",
    "\\theta_\\text{target} = \\rho \\theta_\\text{target} + (1-\\rho)\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numExp = 1000\n",
    "numUpdates = 10\n",
    "bufferSize = 500 # TODO figure out if this buffer size makes sense\n",
    "batchSize = 50\n",
    "p = 0.5 # action noise parameter\n",
    "polyak = 0.75\n",
    "gamma = 0.5\n",
    "\n",
    "printEvery = 25\n",
    "randomizeDipolarEvery = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the actor and critic, as well as target actor and target critic. The actor learns the policy function\n",
    "$$\n",
    "\\pi_\\theta: S \\to A, s \\mapsto a\n",
    "$$\n",
    "that picks the optimal action $a$ for a given state $s$, with some set of parameters $\\theta$. The critic learns the Q-function\n",
    "$$\n",
    "Q_\\phi: S \\times A \\to \\mathbf{R}, (s,a) \\mapsto q\n",
    "$$\n",
    "where $q$ is the total expected rewards by doing action $a$ on a state $s$, and $\\phi$ is the parameter set for the Q-function model. The target actor/critic have different parameter sets $\\theta_\\text{target}$ and $\\phi_\\text{target}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDim = 3 # state represented by sequences of actions...\n",
    "aDim = 3 # action = [phi, rot, time]\n",
    "\n",
    "actor = rlp.Actor(sDim,aDim,None)\n",
    "actorTarget = rlp.Actor(sDim,aDim,None)\n",
    "critic = rlp.Critic(sDim,aDim,None, gamma)\n",
    "criticTarget = rlp.Critic(sDim,aDim,None, gamma)\n",
    "env = rlp.Environment(N, dim, sDim, HWHH0, X, Y)\n",
    "\n",
    "actorTarget.setParams(actor.getParams())\n",
    "criticTarget.setParams(critic.getParams())\n",
    "\n",
    "replayBuffer = rlp.ReplayBuffer(bufferSize)\n",
    "\n",
    "def actionNoise(p):\n",
    "    '''Add noise to actions. Generates a 1x3 array with random values\n",
    "    \n",
    "    Arguments:\n",
    "        p: Parameter to control amount of noise\n",
    "    '''\n",
    "#     return np.array([1.0/4*np.random.choice([0,1,-1],p=[1-p,p/2,p/2]), \\\n",
    "#                      1.0/4*np.random.choice([0,1,-1],p=[1-p,p/2,p/2]), \\\n",
    "#                      np.random.normal(0,.25)])\n",
    "    return np.array([np.random.normal(0, p/2), \\\n",
    "                     np.random.normal(0, p/2), \\\n",
    "                     np.random.normal(0, p/2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implementation of the DDPG algorithm (see [this OpenAI resource for reference](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rMat = np.zeros((numExp,))\n",
    "aMat = np.zeros((numExp,aDim))\n",
    "timeMat = np.zeros((numExp, 2)) # record length of episode so far and number of pulses\n",
    "# keep track of when resets/updates happen\n",
    "resetStateEps = []\n",
    "updateEps = []\n",
    "\n",
    "for i in range(numExp):\n",
    "    if i % printEvery == 0:\n",
    "        print(\"on episode {}\".format(i))\n",
    "    if i > 0 and i % randomizeDipolarEvery == 0:\n",
    "        # randomize dipolar coupling strengths for Hint\n",
    "        Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "    s = env.getState()\n",
    "    a = rlp.clipAction(actor.predict(env.state) + actionNoise(p))\n",
    "    aMat[i,:] = a\n",
    "    env.evolve(a, Hint)\n",
    "    r = env.reward()\n",
    "    rMat[i] = r\n",
    "    timeMat[i,0] = [env.t, np.sum(env.state[:,2] != 0)]\n",
    "    if r > 1:\n",
    "        print(\"high reward in episode {}\".format(i))\n",
    "    s1 = env.getState()\n",
    "    d = env.isDone()\n",
    "    replayBuffer.add(s,a,r,s1,d)\n",
    "    if d:\n",
    "        print(\"terminal state (episode {})\".format(i))\n",
    "        env.reset()\n",
    "        resetStateEps.append(i)\n",
    "    if (i > 0) and (i % 50 == 0):\n",
    "        print(\"updating actor/critic networks (episode {})\".format(i))\n",
    "        updateEps.append(i)\n",
    "        for update in range(numUpdates):\n",
    "            batch = replayBuffer.getSampleBatch(batchSize)\n",
    "            # train critic\n",
    "            critic.trainStep(batch, actorTarget, criticTarget)\n",
    "            # train actor\n",
    "            actor.trainStep(batch, critic)\n",
    "            # update target networks\n",
    "            criticTarget.updateParams(critic, polyak)\n",
    "            actorTarget.updateParams(actor, polyak)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rMat, color='black', label='rewards')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.title('Rewards for each episode')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist(rMat, bins=20, color='black', label='rewards')\n",
    "# ymin, ymax = plt.ylim()\n",
    "# plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.title('Rewards histogram')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the state with highest reward in the buffer (might not be highest of all episodes because buffer forgets)\n",
    "rBuffer = [_[2] for _  in replayBuffer.buffer]\n",
    "rlp.printAction(replayBuffer.buffer[np.argmax(rBuffer)][3])\n",
    "print(\"max reward in buffer: \", np.max(rBuffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging purposes...\n",
    "\n",
    "#rlp.printAction(replayBuffer.buffer[-2][3])\n",
    "\n",
    "Uexp = spla.expm(-1j*(Hint*.11e-6 + Y*2*np.pi))\n",
    "Utarget = ss.getPropagator(HWHH0, .11e-6)\n",
    "ss.fidelity(Utarget, Uexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
