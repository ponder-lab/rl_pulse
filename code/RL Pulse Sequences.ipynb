{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design Using Reinforcement Learning\n",
    "\n",
    "Implementing deep deterministic policy gradient (DDPG) to learn pulse sequence design for spin systems. The [OpenAI SpinningUp resource](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode) has a good theoretical background on DDPG which I used to implement the algorithm below.\n",
    "\n",
    "DDPG is designed for _continuous_ action spaces, which is the ultimate goal for this project (to apply pulses with arbitrary axes of rotation, rotation angles, and times, instead of limiting to pi/2 pulses along X or Y). However, that means the algorithm is less suited to constrained versions of the problem, such as only applying pi/2 pulses of a certain length about X or Y.\n",
    "\n",
    "For training, the following reward function was used\n",
    "$$\n",
    "r = -\\log\\left( 1- \\left| \\text{Tr}\\left( \\frac{U_\\text{target}^\\dagger U_\\text{exp}}{2^N} \\right) \\right| \\right)\n",
    "= -\\log\\left( 1- \\text{fidelity}(U_\\text{target}^\\dagger, U_\\text{exp}) \\right)\n",
    "$$\n",
    "For example, if the fidelity is $0.999$, then the reward $r = -\\log(0.001) = 3$. \n",
    "\n",
    "<!-- For the policy function, I need to perform gradient ascent with the following gradient\n",
    "$$\n",
    "\\nabla_\\theta 1/|B| \\sum_{s \\in B} Q_\\phi (s, \\pi_\\theta(s))\n",
    "$$\n",
    "\n",
    "And for the Q-function, perform gradient descent with\n",
    "$$\n",
    "\\nabla_\\phi 1/|B| \\sum_{(s,a,r,s',d) \\in B} (Q_\\phi(s,a) - y(r,s',d))^2\n",
    "$$ -->\n",
    "\n",
    "Other resources:\n",
    "\n",
    "- https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function\n",
    "- https://www.tensorflow.org/guide/migrate#customize_the_training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinSimulation as ss\n",
    "import rlPulse as rlp\n",
    "import numpy as np\n",
    "import scipy.linalg as spla\n",
    "import importlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ss)\n",
    "importlib.reload(rlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize spin system\n",
    "\n",
    "This sets the parameters of the system ($N$ spin-1/2 particles, which corresponds to a Hilbert space with dimension $2^N$). For the purposes of simulation, $\\hbar \\equiv 1$.\n",
    "\n",
    "The total internal Hamiltonian is given by\n",
    "$$\n",
    "H_\\text{int} = C H_\\text{dip} + \\Delta \\sum_i^N I_z^{(i)}\n",
    "$$\n",
    "where $C$ is the coupling strength, $\\Delta$ is the chemical shift strength (each spin is assumed to be identical), and $H_\\text{dip}$ is given by\n",
    "$$\n",
    "H_\\text{dip} = \\sum_{i,j}^N d_{i,j} \\left(3I_z^{(i)}I_z^{(j)} - \\mathbf{I}^{(i)} \\cdot \\mathbf{I}^{(j)}\\right)\n",
    "$$\n",
    "\n",
    "The WAHUHA pulse sequence is designed to remove the dipolar interaction term from the internal Hamiltonian. The pulse sequence is $\\tau, P_{-x}, \\tau, P_{y}, \\tau, \\tau, P_{-y}, \\tau, P_{x}, \\tau$.\n",
    "The zeroth-order average Hamiltonian for the WAHUHA pulse sequence is\n",
    "$$\n",
    "H_\\text{WHH} = \\Delta / 3 \\sum_i^N I_x^{(i)} + I_y^{(i)} + I_z^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "dim = 2**N\n",
    "\n",
    "# pulse = .25e-6    # duration of pulse\n",
    "# delay = 3e-6      # duration of delay\n",
    "coupling = 5e3    # coupling strength\n",
    "delta = 500       # chemical shift strength (for identical spins)\n",
    "\n",
    "(x,y,z) = (ss.x, ss.y, ss.z)\n",
    "(X,Y,Z) = ss.getTotalSpin(N, dim)\n",
    "\n",
    "Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "HWHH0 = ss.getHWHH0(X,Y,Z,delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize RL algorithm\n",
    "\n",
    "An \"action\" performed on the system corresponds to an RF-pulse applied to the system. A pulse can be parametrized by the axis of rotation (e.g. $(\\theta, \\phi)$, but for now $\\theta = \\pi/2$ is assumed so the axis of rotation lies in the xy-plane), the rotation angle, and the duration of the pulse.\n",
    "\n",
    "The state of the system can correspond to the propagator, but because the propagator grows exponentially (it has $4^N$ elements for an $N$-spin system) and the pulse sequence determines the propagator, the state is represented by the pulse sequence instead.\n",
    "\n",
    "The target network parameters $\\theta_\\text{target}$ are updated by\n",
    "$$\n",
    "\\theta_\\text{target} = \\rho \\theta_\\text{target} + (1-\\rho)\\theta\n",
    "$$\n",
    "\n",
    "TODO figure out if this buffer size makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDim = 3 # state represented by sequences of actions...\n",
    "aDim = 3 # action = [phi, rot, time]\n",
    "\n",
    "numExp = 2000 # how many experiences to \"play\" through and learn from\n",
    "bufferSize = 500 # size of the replay buffer (i.e. how many experiences to keep in memory).\n",
    "batchSize = 50 # size of batch (subset of replay buffer) to use as training for actor and critic.\n",
    "p = 1 # action noise parameter\n",
    "polyak = 0.75 # polyak averaging parameter\n",
    "gamma = 0.5 # future reward discount rate\n",
    "\n",
    "printEvery = 50\n",
    "updateAfter = 500 # start updating actor/critic networks after this many episodes\n",
    "updateEvery = 50  # update networks every __ episodes\n",
    "numUpdates = 2 # how many training updates to perform on a random subset of experiences (s,a,r,s1,d)\n",
    "randomizeDipolarEvery = 10\n",
    "lowerNoiseAfter = 500\n",
    "\n",
    "# calculate beta, the weight to adjust fidelity values at different times\n",
    "# beta = 1/10e-6 * np.log(1+1e-6)\n",
    "\n",
    "pDiff = (0-p)/(numExp-lowerNoiseAfter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the actor and critic, as well as target actor and target critic. The actor learns the policy function\n",
    "$$\n",
    "\\pi_\\theta: S \\to A, s \\mapsto a\n",
    "$$\n",
    "that picks the optimal action $a$ for a given state $s$, with some set of parameters $\\theta$ (in this case weights/biases in the neural network). The critic learns the Q-function\n",
    "$$\n",
    "Q_\\phi: S \\times A \\to \\mathbf{R}, (s,a) \\mapsto q\n",
    "$$\n",
    "where $q$ is the total expected rewards by doing action $a$ on a state $s$, and $\\phi$ is the parameter set for the Q-function model. The target actor/critic have different parameter sets $\\theta_\\text{target}$ and $\\phi_\\text{target}$.\n",
    "\n",
    "The \"environment\" keeps track of the system state, and calculates rewards after each episode.\n",
    "\n",
    "The replay buffer keeps track of the most recent episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = rlp.Actor(sDim,aDim,None)\n",
    "actorTarget = rlp.Actor(sDim,aDim,None)\n",
    "critic = rlp.Critic(sDim,aDim,None, gamma)\n",
    "criticTarget = rlp.Critic(sDim,aDim,None, gamma)\n",
    "env = rlp.Environment(N, dim, sDim, HWHH0, X, Y)\n",
    "\n",
    "actorTarget.setParams(actor.getParams())\n",
    "criticTarget.setParams(critic.getParams())\n",
    "\n",
    "replayBuffer = rlp.ReplayBuffer(bufferSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rMat = np.zeros((numExp,))\n",
    "aMat = np.zeros((numExp,aDim))\n",
    "timeMat = np.zeros((numExp, 2)) # record length of episode so far and number of pulses\n",
    "# keep track of when resets/updates happen\n",
    "resetStateEps = []\n",
    "updateEps = []\n",
    "\n",
    "for i in tqdm(range(numExp)):\n",
    "    # randomize dipolar coupling strengths for Hint\n",
    "    if i > 0 and i % randomizeDipolarEvery == 0:\n",
    "        Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "    \n",
    "    s = env.getState()\n",
    "    # get action based on current state and some level of noise\n",
    "    a = rlp.clipAction(actor.predict(env.state) + rlp.actionNoise(p))\n",
    "    # evolve state based on action\n",
    "    env.evolve(a, Hint)\n",
    "    # get reward\n",
    "    r = env.reward()\n",
    "    \n",
    "    # record episode data\n",
    "    aMat[i,:] = a\n",
    "    rMat[i] = r\n",
    "    timeMat[i,:] = [env.t, np.sum(env.state[:,2] != 0)]\n",
    "    \n",
    "    # get updated state, and whether it's a terminal state\n",
    "    s1 = env.getState()\n",
    "    d = env.isDone()\n",
    "    replayBuffer.add(s,a,r,s1,d)\n",
    "    \n",
    "    # update noise parameter\n",
    "    if i > lowerNoiseAfter:\n",
    "        p += pDiff\n",
    "    \n",
    "    # CHECK IF TERMINAL\n",
    "    if d:\n",
    "        env.reset()\n",
    "        resetStateEps.append(i)\n",
    "    # UPDATE NETWORKS\n",
    "    if (i > updateAfter) and (i % updateEvery == 0):\n",
    "#         print(\"updating actor/critic networks (episode {})\".format(i))\n",
    "        updateEps.append(i)\n",
    "        for update in range(numUpdates):\n",
    "            batch = replayBuffer.getSampleBatch(batchSize)\n",
    "            # train critic\n",
    "            critic.trainStep(batch, actorTarget, criticTarget)\n",
    "            # train actor\n",
    "            actor.trainStep(batch, critic)\n",
    "            # update target networks\n",
    "            criticTarget.updateParams(critic, polyak)\n",
    "            actorTarget.updateParams(actor, polyak)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "See how the rewards and actions change over time as the actor/critic (hopefully) learn how to control the spin system.\n",
    "\n",
    "Looking at the histogram below, most rewards are very small (which makes sense, considering a large subset of rewards are from random actions to begin with). There is a slight bump of rewards centered around 3 (corresponding to fidelities of around 0.999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(rMat, bins=20, color='black', label='rewards')\n",
    "plt.title('Rewards histogram')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rewards increase as the algorithm learns (which is a good thing!), but seem to plateau near 4 once the algorithm converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(rMat, 'ok', label='rewards')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.title('Rewards for each episode')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the actions performed on the system. Random noise is added to actions for early episodes (to explore the action space), and gradually less noise is added as the algorithm converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,0], 'ok', label='phi')\n",
    "plt.title('Phi action')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Phi action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,1], 'ok', label='rot')\n",
    "plt.title('Rot action')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Rot action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,2], 'ok', label='time')\n",
    "plt.title('Time action')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Time action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(timeMat[:,0], 'ok', label='time')\n",
    "plt.title('Pulse sequence length (time)')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Pulse sequence length (s)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(timeMat[:,1], 'ok', label='time')\n",
    "plt.title('Pulse sequence length (number of pulses)')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Number of pulses')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below prints the pulse sequences with the highest rewards in the replay buffer. It seems like the algorithm is only learning to apply a single pulse that rotates $2\\pi$ about the x-axis. This pulse \"sequence\" has reasonably high fidelity ($0.99998$), but is clearly not the WAHUHA sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rBuffer = np.array([_[2] for _  in replayBuffer.buffer])\n",
    "indices = rBuffer.argsort()\n",
    "for i in range(1,5):\n",
    "#     print(replayBuffer.buffer[indices[-i]])\n",
    "    rlp.printAction(replayBuffer.buffer[indices[-i]][3])\n",
    "    print(\"reward: \", round(rBuffer[indices[-i]],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate average fidelity for a pulse sequence\n",
    "\n",
    "fidelities = np.zeros((50,))\n",
    "for i in range(50):\n",
    "    Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "    Uexp = rlp.actionToPropagator(N, dim, np.array([0, 1, .5]), Hint, X, Y)\n",
    "    Utarget = ss.getPropagator(HWHH0, (7.11+3.47+10)*1e-6)\n",
    "    fidelities[i] = ss.fidelity(np.power(Utarget,1), np.power(Uexp,1))\n",
    "print(\"mean fidelity: \", np.mean(fidelities), \"std dev: \", np.std(fidelities), \"max: \", np.max(fidelities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
