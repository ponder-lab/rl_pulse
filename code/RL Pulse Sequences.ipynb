{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design Using Reinforcement Learning\n",
    "\n",
    "{{ explanation for my approach, what I'm trying to do }}\n",
    "\n",
    "Currently looking to implement DDPG for RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [OpenAI SpinningUp resource](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode) for the theoretical background on DDPG, and lots of TensorFlow documentation for how to write the algorithm below.\n",
    "\n",
    "For the policy function, I need to perform gradient ascent with the following gradient\n",
    "$$\n",
    "\\nabla_\\theta 1/|B| \\sum_{s \\in B} Q_\\phi (s, \\pi_\\theta(s))\n",
    "$$\n",
    "\n",
    "And for the Q-function, perform gradient descent with\n",
    "$$\n",
    "\\nabla_\\phi 1/|B| \\sum_{(s,a,r,s',d) \\in B} (Q_\\phi(s,a) - y(r,s',d))^2\n",
    "$$\n",
    "\n",
    "Other resources:\n",
    "\n",
    "- https://keras.io/getting-started/sequential-model-guide/\n",
    "- https://www.tensorflow.org/guide/keras/overview\n",
    "- https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function\n",
    "- https://github.com/floodsung/DDPG/blob/master/actor_network.py\n",
    "- https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG.py\n",
    "\n",
    "Also helpful: https://www.tensorflow.org/guide/migrate#customize_the_training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinSimulation as ss\n",
    "import rlPulse as rlp\n",
    "import numpy as np\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ss)\n",
    "importlib.reload(rlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define system parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "dim = 2**N\n",
    "\n",
    "pulse = .25e-6    # duration of pulse\n",
    "delay = 3e-6      # duration of delay\n",
    "f1 = 1/(4*pulse)  # for pi/2 pulses\n",
    "coupling = 5e3    # coupling strength\n",
    "Delta = 500       # chemical shift strength (for identical spins)\n",
    "\n",
    "a = ss.getRandDip(N) # random dipolar coupling strengths\n",
    "(x,y,z) = (ss.x, ss.y, ss.z)\n",
    "(X,Y,Z) = ss.getCollectiveObservables(N, dim)\n",
    "\n",
    "Hdip = ss.getHdip(N, dim, x, y, z, a)\n",
    "Hint = ss.getHint(Hdip, coupling, Z, Delta)\n",
    "HWHH0 = ss.getHWHH0(X,Y,Z,Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the RL algorithm parameters.\n",
    "\n",
    "- `numExp`: Specifies how many experiences to \"play\" through.\n",
    "- `numUpdates`: How many updates to perform using a random subset of experiences from the replay buffer.\n",
    "- `bufferSize`: Size of the replay buffer (i.e. how many experiences to keep in memory).\n",
    "- `batchSize`: Size of batch (subset of replay buffer) to use as training for actor and critic.\n",
    "- `p`: Action noise level (determines probabilities of rotating along a different axis or by different angle)\n",
    "- `polyak`: Polyak averaging parameter. The target network parameters $\\theta_\\text{target}$ are updated by\n",
    "$$\n",
    "\\theta_\\text{target} = \\rho \\theta_\\text{target} + (1-\\rho)\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numExp = 500\n",
    "numUpdates = 5\n",
    "bufferSize = 500 # TODO figure out if this buffer size makes sense\n",
    "batchSize = 50\n",
    "p = 0.5 # action noise level\n",
    "polyak = 0.75\n",
    "gamma = 0.5\n",
    "\n",
    "printEvery = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the actor and critic, as well as target actor and target critic. The actor learns the policy function\n",
    "$$\n",
    "\\pi_\\theta: S \\to A, s \\mapsto a\n",
    "$$\n",
    "that picks the optimal action $a$ for a given state $s$, with some set of parameters $\\theta$. The critic learns the Q-function\n",
    "$$\n",
    "Q_\\phi: S \\times A \\to \\mathbf{R}, (s,a) \\mapsto q\n",
    "$$\n",
    "where $q$ is the total expected rewards by doing action $a$ on a state $s$, and $\\phi$ is the parameter set for the Q-function model. The target actor/critic have different parameter sets $\\theta_\\text{target}$ and $\\phi_\\text{target}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDim = 3 # state represented by sequences of actions...\n",
    "aDim = 3 # action = [phi, rot, time]\n",
    "\n",
    "actor = rlp.Actor(sDim,aDim,None)\n",
    "actorTarget = rlp.Actor(sDim,aDim,None)\n",
    "critic = rlp.Critic(sDim,aDim,None, gamma)\n",
    "criticTarget = rlp.Critic(sDim,aDim,None, gamma)\n",
    "env = rlp.Environment(N, dim, sDim, HWHH0, X, Y)\n",
    "\n",
    "replayBuffer = rlp.ReplayBuffer(bufferSize)\n",
    "\n",
    "def actionNoise(p):\n",
    "    '''Add noise to actions. Generates a 1x3 array with random values\n",
    "    \n",
    "    Arguments:\n",
    "        p: Parameter to control level of noise\n",
    "    '''\n",
    "    #return np.random.uniform([0,0,-1e6*p], [2*np.pi*p, 2*np.pi*p, 1e-6*p]) # fully unconstrained noise\n",
    "    # constrained noise: randomly change the axis or rotation angle\n",
    "    return np.array([1.0/4*np.random.choice([0,1,-1],p=[1-p,p/2,p/2]), \\\n",
    "                     1.0/4*np.random.choice([0,1,-1],p=[1-p,p/2,p/2]), \\\n",
    "                     np.random.normal(0,.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implementation of the DDPG algorithm (see [this OpenAI resource for reference](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actorTarget.setParams(actor.getParams())\n",
    "criticTarget.setParams(critic.getParams())\n",
    "\n",
    "# keep track of when resets/updates happen\n",
    "resetStateEps = []\n",
    "updateEps = []\n",
    "\n",
    "for i in range(numExp):\n",
    "    if i % printEvery == 0:\n",
    "        print(\"on episode {}\".format(i))\n",
    "    s = env.getState()\n",
    "    a = rlp.clipAction(actor.predict(env.state) + actionNoise(p))\n",
    "    env.evolve(a, Hint)\n",
    "    r = env.reward()\n",
    "    if r > 1:\n",
    "        print(\"high reward in episode {}\".format(i))\n",
    "    s1 = env.getState()\n",
    "    d = env.isDone()\n",
    "    replayBuffer.add(s,a,r,s1,d)\n",
    "    if d:\n",
    "        print(\"terminal state (episode {})\".format(i))\n",
    "        env.reset()\n",
    "        resetStateEps.append(i)\n",
    "    if (i > 0) and (i % 50 == 0):\n",
    "        print(\"updating actor/critic networks (episode {})\".format(i))\n",
    "        updateEps.append(i)\n",
    "        for update in range(numUpdates):\n",
    "            batch = replayBuffer.getSampleBatch(batchSize)\n",
    "            # train critic\n",
    "            critic.trainStep(batch, actorTarget, criticTarget)\n",
    "            # train actor\n",
    "            actor.trainStep(batch, critic)\n",
    "            # update target networks\n",
    "            criticTarget.updateParams(critic, polyak)\n",
    "            actorTarget.updateParams(actor, polyak)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rMat, color='black', label='rewards')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.title('Rewards')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging purposes...\n",
    "\n",
    "#print(s0)\n",
    "#print(actor.predict(s0))\n",
    "print(actor.predict(np.zeros((1,4,3))))\n",
    "#print(actorTarget.predict(replayBuffer.buffer[264][0]))\n",
    "\n",
    "#rMat[150:156]\n",
    "print(replayBuffer.buffer[155])\n",
    "#s0 = replayBuffer.buffer[62][0]\n",
    "\n",
    "a = [0.21712768, 0.16245341, 0.86427754]\n",
    "print(ss.fidelity(rlp.actionToPropagator(N, dim, a, Hint, X, Y), \\\n",
    "            ss.getPropagator(HWHH0, rlp.getActionTime(a))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
