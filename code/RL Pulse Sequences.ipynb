{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design Using Reinforcement Learning\n",
    "\n",
    "Implementing deep deterministic policy gradient (DDPG) to learn pulse sequence design for spin systems. The [OpenAI SpinningUp resource](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode) has a good theoretical background on DDPG which I used to implement the algorithm below.\n",
    "\n",
    "DDPG is designed for _continuous_ action spaces, which is the ultimate goal for this project (to apply pulses with arbitrary axes of rotation, rotation angles, and times, instead of limiting to pi/2 pulses along X or Y). However, that means the algorithm is less suited to constrained versions of the problem, such as only applying pi/2 pulses of a certain length about X or Y.\n",
    "\n",
    "For training, the following reward function was used\n",
    "$$\n",
    "r = -\\log\\left( 1- \\left| \\text{Tr}\\left( \\frac{U_\\text{target}^\\dagger U_\\text{exp}}{2^N} \\right) \\right| \\right)\n",
    "= -\\log\\left( 1- \\text{fidelity}(U_\\text{target}, U_\\text{exp}) \\right)\n",
    "$$\n",
    "For example, if the fidelity is $0.999$, then the reward $r = -\\log(0.001) = 3$. \n",
    "\n",
    "<!-- For the policy function, I need to perform gradient ascent with the following gradient\n",
    "$$\n",
    "\\nabla_\\theta 1/|B| \\sum_{s \\in B} Q_\\phi (s, \\pi_\\theta(s))\n",
    "$$\n",
    "\n",
    "And for the Q-function, perform gradient descent with\n",
    "$$\n",
    "\\nabla_\\phi 1/|B| \\sum_{(s,a,r,s',d) \\in B} (Q_\\phi(s,a) - y(r,s',d))^2\n",
    "$$ -->\n",
    "\n",
    "Other resources:\n",
    "\n",
    "- https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function\n",
    "- https://www.tensorflow.org/guide/migrate#customize_the_training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinSimulation as ss\n",
    "import rlPulse as rlp\n",
    "import numpy as np\n",
    "import scipy.linalg as spla\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ss)\n",
    "importlib.reload(rlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize spin system\n",
    "\n",
    "This sets the parameters of the system ($N$ spin-1/2 particles, which corresponds to a Hilbert space with dimension $2^N$). For the purposes of simulation, $\\hbar \\equiv 1$.\n",
    "\n",
    "The total internal Hamiltonian is given by\n",
    "$$\n",
    "H_\\text{int} = C H_\\text{dip} + \\Delta \\sum_i^N I_z^{(i)}\n",
    "$$\n",
    "where $C$ is the coupling strength, $\\Delta$ is the chemical shift strength (each spin is assumed to be identical), and $H_\\text{dip}$ is given by\n",
    "$$\n",
    "H_\\text{dip} = \\sum_{i,j}^N d_{i,j} \\left(3I_z^{(i)}I_z^{(j)} - \\mathbf{I}^{(i)} \\cdot \\mathbf{I}^{(j)}\\right)\n",
    "$$\n",
    "\n",
    "The WAHUHA pulse sequence is designed to remove the dipolar interaction term from the internal Hamiltonian. The pulse sequence is $\\tau, P_{-x}, \\tau, P_{y}, \\tau, \\tau, P_{-y}, \\tau, P_{x}, \\tau$.\n",
    "The zeroth-order average Hamiltonian for the WAHUHA pulse sequence is\n",
    "$$\n",
    "H_\\text{WHH} = \\Delta / 3 \\sum_i^N I_x^{(i)} + I_y^{(i)} + I_z^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "dim = 2**N\n",
    "\n",
    "# pulse = .25e-6    # duration of pulse\n",
    "# delay = 3e-6      # duration of delay\n",
    "coupling = 5e3    # coupling strength\n",
    "delta = 500       # chemical shift strength (for identical spins)\n",
    "\n",
    "(x,y,z) = (ss.x, ss.y, ss.z)\n",
    "(X,Y,Z) = ss.getTotalSpin(N, dim)\n",
    "\n",
    "Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "HWHH0 = ss.getHWHH0(X,Y,Z,delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize RL algorithm\n",
    "\n",
    "An \"action\" performed on the system corresponds to an RF-pulse applied to the system. A pulse can be parametrized by the axis of rotation (e.g. $(\\theta, \\phi)$, but for now $\\theta = \\pi/2$ is assumed so the axis of rotation lies in the xy-plane), the rotation angle, and the duration of the pulse.\n",
    "\n",
    "The state of the system can correspond to the propagator, but because the propagator grows exponentially (it has $4^N$ elements for an $N$-spin system) and the pulse sequence determines the propagator, the state is represented by the pulse sequence instead.\n",
    "\n",
    "The target network parameters $\\theta_\\text{target}$ are updated by\n",
    "$$\n",
    "\\theta_\\text{target} = (1-\\rho) \\theta_\\text{target} + \\rho\\theta\n",
    "$$\n",
    "\n",
    "TODO figure out if this buffer size makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDim = 3 # state represented by sequences of actions...\n",
    "aDim = 3 # action = [phi, rot, time]\n",
    "learningRate = 0.001 # learning rate for optimizer\n",
    "\n",
    "numExp = 1000 # how many experiences to \"play\" through and learn from\n",
    "bufferSize = 1000 # size of the replay buffer (i.e. how many experiences to keep in memory).\n",
    "batchSize = 100 # size of batch for training\n",
    "polyak = 0.01 # polyak averaging parameter\n",
    "gamma = .5 # future reward discount rate\n",
    "\n",
    "updateAfter = 100 # wait until updating actor/critic networks to fill buffer\n",
    "updateEvery = 10 # wait between updates (faster)\n",
    "numUpdates = 1 # how many training updates to perform on a random subset of experiences (s,a,r,s1,d)\n",
    "testEvery = 250 # how often to evaluate performance on an initial state\n",
    "\n",
    "p = 1 # action noise parameter\n",
    "dp = -p/numExp / (3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the actor and critic, as well as target actor and target critic. The actor learns the policy function\n",
    "$$\n",
    "\\pi_\\theta: S \\to A, s \\mapsto a\n",
    "$$\n",
    "that picks the optimal action $a$ for a given state $s$, with some set of parameters $\\theta$ (in this case weights/biases in the neural network). The critic learns the Q-function\n",
    "$$\n",
    "Q_\\phi: S \\times A \\to \\mathbf{R}, (s,a) \\mapsto q\n",
    "$$\n",
    "where $q$ is the total expected rewards by doing action $a$ on a state $s$, and $\\phi$ is the parameter set for the Q-function model. The target actor/critic have different parameter sets $\\theta_\\text{target}$ and $\\phi_\\text{target}$.\n",
    "\n",
    "The \"environment\" keeps track of the system state, and calculates rewards after each episode.\n",
    "\n",
    "The replay buffer keeps track of the most recent episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = rlp.Actor(sDim,aDim, learningRate, 4, 4)\n",
    "actorTarget = rlp.Actor(sDim,aDim, learningRate, 4, 4)\n",
    "critic = rlp.Critic(sDim, aDim, gamma, learningRate, 4, 4)\n",
    "criticTarget = rlp.Critic(sDim, aDim, gamma, learningRate, 4, 4)\n",
    "env = rlp.Environment(N, dim, sDim, HWHH0, X, Y)\n",
    "\n",
    "actorTarget.setParams(actor.getParams())\n",
    "criticTarget.setParams(critic.getParams())\n",
    "\n",
    "replayBuffer = rlp.ReplayBuffer(bufferSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# record actions and rewards from learning\n",
    "actorAMat = np.zeros((numExp,aDim))\n",
    "aMat = np.zeros((numExp,aDim))\n",
    "timeMat = np.zeros((numExp, 2)) # duration of sequence and number of pulses\n",
    "rMat = np.zeros((numExp,))\n",
    "# record when resets/updates happen\n",
    "resetStateEps = []\n",
    "updateEps = [] # TODO remove this\n",
    "# and record parameter differences between networks and targets (episode #, actor, critic)\n",
    "paramDistance = []\n",
    "\n",
    "# record test results: episode, final pulse sequence (to terminal state), rewards at each episode\n",
    "testResults = []\n",
    "isTesting = False\n",
    "\n",
    "numActions = 0\n",
    "\n",
    "for i in tqdm(range(numExp)):\n",
    "    s = env.getState()\n",
    "    # get action based on current state and some level of noise\n",
    "    actorA = actor.predict(env.state)\n",
    "    if not isTesting:\n",
    "        aNoise = rlp.actionNoise(p)\n",
    "        a = rlp.clipAction(actorA + aNoise)\n",
    "    else:\n",
    "        a = rlp.clipAction(actorA)\n",
    "    \n",
    "    # update noise parameter\n",
    "    p = np.maximum(p + dp, .2)\n",
    "    \n",
    "    numActions += 1\n",
    "    \n",
    "    # evolve state based on action\n",
    "    env.evolve(a, Hint)\n",
    "    # get reward\n",
    "    r = env.reward2()\n",
    "    \n",
    "    # get updated state, and whether it's a terminal state\n",
    "    s1 = env.getState()\n",
    "    d = env.isDone()\n",
    "    replayBuffer.add(s,a,r,s1,d)\n",
    "    \n",
    "    # record episode data\n",
    "    aMat[i,:] = a\n",
    "    actorAMat[i,:] = actorA\n",
    "    rMat[i] = r\n",
    "    timeMat[i,:] = [env.t, numActions]\n",
    "    \n",
    "    if i % int(numExp/25) == 0:\n",
    "        # calculate distance between parameters for actors/critics\n",
    "        paramDistance.append((i, actor.paramDistance(actorTarget), \\\n",
    "                                 critic.paramDistance(criticTarget)))\n",
    "    \n",
    "    # if the state is terminal\n",
    "    if d:\n",
    "        if isTesting:\n",
    "            # record results from the test and go back to learning\n",
    "            testResults.append((i, s1, rMat[(i-numActions+1):(i+1)]))\n",
    "            isTesting = not isTesting\n",
    "        else:\n",
    "            # check if it's time to test performance\n",
    "            if len(testResults)*testEvery < i:\n",
    "                isTesting = True\n",
    "        \n",
    "        # randomize dipolar coupling strengths for Hint\n",
    "        Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "        # reset environment\n",
    "        env.reset()\n",
    "        resetStateEps.append(i)\n",
    "        numActions = 0\n",
    "    \n",
    "    # update networks\n",
    "    if i > updateAfter and i % updateEvery == 0:\n",
    "        updateEps.append(i)\n",
    "        for update in range(numUpdates):\n",
    "            batch = replayBuffer.getSampleBatch(batchSize)\n",
    "            # train critic\n",
    "            critic.trainStep(batch, actorTarget, criticTarget)\n",
    "            # train actor\n",
    "            actor.trainStep(batch, critic)\n",
    "            # update target networks\n",
    "            criticTarget.copyParams(critic, polyak)\n",
    "            actorTarget.copyParams(actor, polyak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "See how the rewards and actions change over time as the actor/critic (hopefully) learn how to control the spin system.\n",
    "\n",
    "Looking at the histogram below, most rewards are very small (which makes sense, considering a large subset of rewards are from random actions to begin with). There is a slight bump of rewards centered around 3 (corresponding to fidelities of around 0.999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist(rMat, bins=20, color='black', label='rewards')\n",
    "plt.title('Rewards histogram')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rewards increase as the algorithm learns (which is a good thing!), but seem to plateau near 4 once the algorithm converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(rMat, 'ok', label='rewards')\n",
    "ymin, ymax = plt.ylim()\n",
    "# plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.title('Rewards for each episode')\n",
    "plt.xlabel('Episode number')\n",
    "# plt.yscale('log')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the actions performed on the system. Random noise is added to actions for early episodes (to explore the action space), and gradually less noise is added as the algorithm converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,0], 'ok', label='phi', zorder=1)\n",
    "plt.plot(actorAMat[:,0], '.b', label='phi (actor)', zorder=2)\n",
    "plt.title('Phi action')\n",
    "ymin, ymax = plt.ylim()\n",
    "# plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Phi action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,1], 'ok', label='rot')\n",
    "plt.plot(actorAMat[:,1], '.b', label='rot (actor)', zorder=2)\n",
    "plt.title('Rot action')\n",
    "ymin, ymax = plt.ylim()\n",
    "# plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Rot action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(aMat[:,2], 'ok', label='time')\n",
    "plt.plot(actorAMat[:,2], '.b', label='time (actor)', zorder=2)\n",
    "plt.title('Time action')\n",
    "ymin, ymax = plt.ylim()\n",
    "# plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "# plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Time action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(timeMat[:,0], 'ok', label='time')\n",
    "plt.title('Pulse sequence length (time)')\n",
    "ymin, ymax = plt.ylim()\n",
    "# plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Pulse sequence length (s)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(timeMat[:,1], 'ok', label='time')\n",
    "plt.title('Pulse sequence length (number of pulses)')\n",
    "ymin, ymax = plt.ylim()\n",
    "# plt.vlines(updateEps, ymin, ymax, color='red', alpha=0.2, label='updates')\n",
    "#plt.vlines(resetStateEps, ymin, ymax, color='blue', alpha=0.2, linestyles='dashed', label='state reset')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Number of pulses')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below prints the pulse sequences with the highest rewards in the replay buffer. It seems like the algorithm is only learning to apply a single pulse that rotates $2\\pi$ about the x-axis. This pulse \"sequence\" has reasonably high fidelity ($0.99998$), but is clearly not the WAHUHA sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rBuffer = np.array([_[2] for _  in replayBuffer.buffer])\n",
    "# indSorted = rBuffer.argsort()\n",
    "# for i in range(1,5):\n",
    "#     print(\"Highest rewards in buffer (#{})\\n\".format(i))\n",
    "#     print(\"Index in buffer: {}\\n\".format(indSorted[-i]))\n",
    "#     sequence = replayBuffer.buffer[indSorted[-i]][3] # sequence of actions\n",
    "#     print(rlp.formatAction(sequence) + \"\\n\")\n",
    "#     # calculate mean fidelity from ensemble of dipolar couplings\n",
    "#     fidelities = np.zeros((10,))\n",
    "#     t = np.sum(rlp.getTimeFromAction(sequence))\n",
    "#     for i in range(10):\n",
    "#         Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "#         Uexp = rlp.getPropagatorFromAction(N, dim, sequence, Hint, X, Y)\n",
    "#         Utarget = ss.getPropagator(HWHH0, t)\n",
    "#         fidelities[i] = ss.fidelity(Utarget, Uexp)\n",
    "#     fMean = np.mean(fidelities)\n",
    "#     print(f\"Mean fidelity: {fMean}\")\n",
    "#     r = -1*np.log10(1+1e-12-fMean**(20e-6/t))\n",
    "#     print(f\"Reward: {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the test results (no noise added to the actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in testResults:\n",
    "    print(f\"Test result from episode {result[0]}\\n\\nChosen pulse sequence:\")\n",
    "    rlp.printAction(result[1])\n",
    "    print(f\"Rewards from the pulse sequence:\\n{result[2]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in paramDistance:\n",
    "    print(f\"episode {i[0]}:\\tactor diff={i[1]:0.2},\\tcritic diff={i[2]:0.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = np.zeros((16,3), dtype=\"float32\")\n",
    "# # s[0,:] = [0,0,1]\n",
    "# # s[1,:] = [0,.25,.3]\n",
    "# print(actor.predict(s))\n",
    "# a = np.array([0,0,1], dtype=\"float32\")\n",
    "# print(critic.predict(s,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate average fidelity for a pulse sequence\n",
    "# num = 25\n",
    "# fidelities = np.zeros((num,))\n",
    "# for i in range(num):\n",
    "#     Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "#     Uexp = rlp.getPropagatorFromAction(N, dim, np.array([0,1,.3]), Hint, X, Y)\n",
    "#     Utarget = ss.getPropagator(HWHH0, (.1)*1e-6)\n",
    "#     fidelities[i] = ss.fidelity(np.power(Utarget,1), np.power(Uexp,1))\n",
    "# print(\"mean fidelity: \", np.mean(fidelities), \"std dev: \", np.std(fidelities), \"max: \", np.max(fidelities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
