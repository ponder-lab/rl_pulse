{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design Using Reinforcement Learning\n",
    "\n",
    "Implementing deep deterministic policy gradient (DDPG) to learn pulse sequence design for spin systems. The [OpenAI SpinningUp resource](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode) has a good theoretical background on DDPG which I used to implement the algorithm below.\n",
    "\n",
    "DDPG is designed for _continuous_ action spaces, which is the ultimate goal for this project (to apply pulses with arbitrary axes of rotation, rotation angles, and times, instead of limiting to pi/2 pulses along X or Y). However, that means the algorithm is less suited to constrained versions of the problem, such as only applying pi/2 pulses of a certain length about X or Y.\n",
    "\n",
    "For training, the following reward function was used\n",
    "$$\n",
    "r = -\\log\\left( 1- \\left| \\text{Tr}\\left( \\frac{U_\\text{target}^\\dagger U_\\text{exp}}{2^N} \\right) \\right| \\right)\n",
    "= -\\log\\left( 1- \\text{fidelity}(U_\\text{target}, U_\\text{exp}) \\right)\n",
    "$$\n",
    "For example, if the fidelity is $0.999$, then the reward $r = -\\log(0.001) = 3$. \n",
    "\n",
    "<!-- For the policy function, I need to perform gradient ascent with the following gradient\n",
    "$$\n",
    "\\nabla_\\theta 1/|B| \\sum_{s \\in B} Q_\\phi (s, \\pi_\\theta(s))\n",
    "$$\n",
    "\n",
    "And for the Q-function, perform gradient descent with\n",
    "$$\n",
    "\\nabla_\\phi 1/|B| \\sum_{(s,a,r,s',d) \\in B} (Q_\\phi(s,a) - y(r,s',d))^2\n",
    "$$ -->\n",
    "\n",
    "Other resources:\n",
    "\n",
    "- https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#define_the_loss_and_gradient_function\n",
    "- https://www.tensorflow.org/guide/migrate#customize_the_training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spin_simulation as ss\n",
    "import rl_pulse as rlp\n",
    "import numpy as np\n",
    "import scipy.linalg as spla\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ss)\n",
    "importlib.reload(rlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize spin system\n",
    "\n",
    "This sets the parameters of the system ($N$ spin-1/2 particles, which corresponds to a Hilbert space with dimension $2^N$). For the purposes of simulation, $\\hbar \\equiv 1$.\n",
    "\n",
    "The total internal Hamiltonian is given by\n",
    "$$\n",
    "H_\\text{int} = C H_\\text{dip} + \\Delta \\sum_i^N I_z^{(i)}\n",
    "$$\n",
    "where $C$ is the coupling strength, $\\Delta$ is the chemical shift strength (each spin is assumed to be identical), and $H_\\text{dip}$ is given by\n",
    "$$\n",
    "H_\\text{dip} = \\sum_{i,j}^N d_{i,j} \\left(3I_z^{(i)}I_z^{(j)} - \\mathbf{I}^{(i)} \\cdot \\mathbf{I}^{(j)}\\right)\n",
    "$$\n",
    "\n",
    "The WAHUHA pulse sequence is designed to remove the dipolar interaction term from the internal Hamiltonian. The pulse sequence is $\\tau, P_{-x}, \\tau, P_{y}, \\tau, \\tau, P_{-y}, \\tau, P_{x}, \\tau$.\n",
    "The zeroth-order average Hamiltonian for the WAHUHA pulse sequence is\n",
    "$$\n",
    "H_\\text{WHH} = \\Delta / 3 \\sum_i^N I_x^{(i)} + I_y^{(i)} + I_z^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "dim = 2**N\n",
    "coupling = 2*np.pi * 5e3    # coupling strength\n",
    "delta = 2*np.pi * 500       # chemical shift strength (for identical spins)\n",
    "\n",
    "(x,y,z) = (ss.x, ss.y, ss.z)\n",
    "(X,Y,Z) = ss.get_total_spin(N, dim)\n",
    "\n",
    "Hdip, Hint = ss.getAllH(N, dim, coupling, delta)\n",
    "HWHH0 = ss.get_H_WHH_0(N, dim, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize RL algorithm\n",
    "\n",
    "An \"action\" performed on the system corresponds to an RF-pulse applied to the system. A pulse can be parametrized by the axis of rotation (e.g. $(\\theta, \\phi)$, but for now $\\theta = \\pi/2$ is assumed so the axis of rotation lies in the xy-plane), the rotation angle, and the duration of the pulse.\n",
    "\n",
    "The state of the system can correspond to the propagator, but because the propagator grows exponentially (it has $4^N$ elements for an $N$-spin system) and the pulse sequence determines the propagator, the state is represented by the pulse sequence instead.\n",
    "\n",
    "The target network parameters $\\theta_\\text{target}$ are updated by\n",
    "$$\n",
    "\\theta_\\text{target} = (1-\\rho) \\theta_\\text{target} + \\rho\\theta\n",
    "$$\n",
    "\n",
    "TODO figure out if this buffer size makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDim = 3 # state represented by sequences of actions...\n",
    "aDim = 3 # action = [phi, rot, time]\n",
    "\n",
    "numGen = 5 # how many generations to run\n",
    "bufferSize = int(1e5) # size of the replay buffer\n",
    "batchSize = 1024 # size of batch for training, multiple of 32\n",
    "popSize = 10 # size of population\n",
    "polyak = .01 # polyak averaging parameter\n",
    "gamma = .99 # future reward discount rate\n",
    "\n",
    "syncEvery = 1 # how often to copy RL actor into population\n",
    "\n",
    "p = .05\n",
    "\n",
    "actorLR = .01\n",
    "criticLR = .01\n",
    "lstmLayers = 1\n",
    "fcLayers = 3\n",
    "lstmUnits = 32\n",
    "fcUnits = 256\n",
    "\n",
    "eliteFrac = .2\n",
    "tourneyFrac = .3\n",
    "mutateProb = .25\n",
    "mutateFrac = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the actor and critic, as well as target actor and target critic. The actor learns the policy function\n",
    "$$\n",
    "\\pi_\\theta: S \\to A, s \\mapsto a\n",
    "$$\n",
    "that picks the optimal action $a$ for a given state $s$, with some set of parameters $\\theta$ (in this case weights/biases in the neural network). The critic learns the Q-function\n",
    "$$\n",
    "Q_\\phi: S \\times A \\to \\mathbf{R}, (s,a) \\mapsto q\n",
    "$$\n",
    "where $q$ is the total expected rewards by doing action $a$ on a state $s$, and $\\phi$ is the parameter set for the Q-function model. The target actor/critic have different parameter sets $\\theta_\\text{target}$ and $\\phi_\\text{target}$.\n",
    "\n",
    "The \"environment\" keeps track of the system state, and calculates rewards after each episode.\n",
    "\n",
    "The replay buffer keeps track of the most recent episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rlp.Environment(N, dim, coupling, delta, sDim, HWHH0, X, Y)\n",
    "noiseProcess = rlp.NoiseProcess(p)\n",
    "\n",
    "actor = rlp.Actor(sDim,aDim, actorLR)\n",
    "critic = rlp.Critic(sDim, aDim, gamma, criticLR)\n",
    "actor.createNetwork(lstmLayers, fcLayers, lstmUnits, fcUnits)\n",
    "critic.createNetwork(lstmLayers, fcLayers, lstmUnits, fcUnits)\n",
    "\n",
    "actorTarget = actor.copy()\n",
    "criticTarget = critic.copy()\n",
    "\n",
    "pop = rlp.Population(popSize)\n",
    "pop.startPopulation(sDim, aDim, actorLR, \\\n",
    "    lstmLayers, fcLayers, lstmUnits, fcUnits)\n",
    "\n",
    "replayBuffer = rlp.ReplayBuffer(bufferSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paramDiff = []\n",
    "popFitnesses = [] # generation, array of fitnesses\n",
    "testMat = [] # generation, fitness from test\n",
    "\n",
    "samples = 250\n",
    "\n",
    "for i in range(numGen):\n",
    "    # evaluate and iterate the population\n",
    "    pop.evaluate(env, replayBuffer, None, numEval=2)\n",
    "    if i % int(np.ceil(numGen / samples)) == 0:\n",
    "        popFitnesses.append((i, np.copy(pop.fitnesses)))\n",
    "    pop.iterate(eliteFrac=eliteFrac, tourneyFrac=tourneyFrac, \\\n",
    "         mutateProb=mutateProb, mutateFrac=mutateFrac)\n",
    "    print(\"iterated population\")\n",
    "    \n",
    "    # evaluate the actor\n",
    "    f = actor.evaluate(env, replayBuffer, noiseProcess)\n",
    "    print(f\"evaluated the actor,\\tfitness is {f:.02f}\")\n",
    "    \n",
    "    # update networks\n",
    "    batch = replayBuffer.getSampleBatch(batchSize)\n",
    "    # train critic\n",
    "    critic.trainStep(batch, actorTarget, criticTarget)\n",
    "    # train actor\n",
    "    actor.trainStep(batch, critic)\n",
    "    # update target networks\n",
    "    criticTarget.copyParams(critic, polyak)\n",
    "    actorTarget.copyParams(actor, polyak)\n",
    "    \n",
    "    print(\"trained actor/critic\")\n",
    "    \n",
    "    if i % int(np.ceil(numGen / samples)) == 0:\n",
    "        print(\"=\"*20 + f\"\\nRecording test results (generation {i})\")\n",
    "        s, rMat = actor.test(env)\n",
    "        f = np.max(rMat)\n",
    "        # record results from the test\n",
    "        print(f'Fitness from test: {f:0.02f}')\n",
    "        testMat.append((i, f))\n",
    "        print(f\"Test result from generation {i}\")\n",
    "        print(\"Chosen pulse sequence:\")\n",
    "        print(rlp.formatAction(s) + \"\\n\")\n",
    "        print(\"Rewards from the pulse sequence:\\n\")\n",
    "        for testR in rMat:\n",
    "            print(f\"{testR:.02f}, \", end='')\n",
    "        print(f'\\nFitness: {f:.02f}')\n",
    "        print(\"\\n\"*3)\n",
    "    \n",
    "    print(f'buffer size is {replayBuffer.size}\\n')\n",
    "    \n",
    "    if i % syncEvery == 0:\n",
    "        # sync actor with population\n",
    "        pop.sync(actor)\n",
    "    \n",
    "    if i % int(np.ceil(numGen/samples)) == 0:\n",
    "        # calculate difference between parameters for actors/critics\n",
    "        paramDiff.append((i, actor.paramDiff(actorTarget), \\\n",
    "                                 critic.paramDiff(criticTarget)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "diffEps = [_[0] for _ in paramDiff]\n",
    "actorDiffs = np.array([_[1] for _ in paramDiff])\n",
    "criticDiffs = np.array([_[2] for _ in paramDiff])\n",
    "\n",
    "for d in range(np.shape(actorDiffs)[1]):\n",
    "    plt.plot(diffEps, actorDiffs[:,d], label=f\"parameter {d}\")\n",
    "plt.title(f\"Actor parameter MSE vs target networks\")\n",
    "plt.xlabel('Generation number')\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')\n",
    "#plt.legend()\n",
    "# plt.gcf().set_size_inches(12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for d in range(np.shape(actorDiffs)[1]):\n",
    "    plt.plot(diffEps, criticDiffs[:,d], label=f\"parameter {d}\")\n",
    "plt.title(f\"Critic parameter MSE vs target networks\")\n",
    "plt.xlabel('Generation number')\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "popFitGens = [_[0] for _ in popFitnesses]\n",
    "popFits = [_[1] for _ in popFitnesses]\n",
    "\n",
    "for i in range(len(popFitGens)):\n",
    "    g = popFitGens[i]\n",
    "    plt.plot([g] * len(popFits[i]), popFits[i], '.k')\n",
    "plt.title(f\"Population fitnesses by generation\")\n",
    "plt.xlabel('Generation number')\n",
    "plt.ylabel('Fitness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "testGens = [_[0] for _ in testMat]\n",
    "testFits = [_[1] for _ in testMat]\n",
    "\n",
    "plt.plot(testGens, testFits, '.k')\n",
    "plt.title(f\"Test fitnesses by generation\")\n",
    "plt.xlabel('Generation number')\n",
    "plt.ylabel('Fitness')\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.zeros((32,3), dtype=\"float32\")\n",
    "s[:,2] = -1\n",
    "# s1 = np.array([[0,0,1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]], dtype=\"float32\")\n",
    "for i in range(5):\n",
    "    print(pop.pop[i].predict(s))\n",
    "# print(actor.predict(s1))\n",
    "# a = np.array([0,0,1], dtype=\"float32\")\n",
    "# a1 = np.array([.5,.5,.5], dtype=\"float32\")\n",
    "# print(critic.predict(s,a))\n",
    "# print(critic.predict(s,a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = actor.getParams()\n",
    "print(len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
