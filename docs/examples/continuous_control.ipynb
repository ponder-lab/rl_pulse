{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design using DDPG\n",
    "_Written by Will Kaufman, October 2020_\n",
    "\n",
    "This notebook walks through a reinforcement learning approach to pulse sequence design for spin systems. [TF-Agents](https://www.tensorflow.org/agents) is used as a reinforcement learning library that uses Tensorflow, a common machine learning framework.\n",
    "\n",
    "The following notebook is loosely based on [this DDPG example](https://github.com/tensorflow/agents/blob/v0.6.0/tf_agents/agents/ddpg/examples/v2/train_eval.py#L71) using TF-Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import qutip as qt\n",
    "import tensorflow as tf\n",
    "\n",
    "from rl_pulse.environments import spin_system_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(spin_system_continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define algorithm hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO eventually fill in hyperparameters at top of doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add summary writers, save data periodically to view in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add global step, for training/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the spin system\n",
    "\n",
    "This sets the parameters of the system ($N$ spin-1/2 particles, which corresponds to a Hilbert space with dimension $2^N$). For the purposes of simulation, $\\hbar \\equiv 1$.\n",
    "\n",
    "The total internal Hamiltonian is given by\n",
    "$$\n",
    "H_\\text{int} = C H_\\text{dip} + \\sum_i^N \\delta_i I_z^{i}\n",
    "$$\n",
    "where $C$ is the coupling strength, $\\delta$ is the chemical shift strength (each spin is assumed to be identical), and $H_\\text{dip}$ is given by\n",
    "$$\n",
    "H_\\text{dip} = \\sum_{i,j}^N d_{i,j} \\left(3I_z^{i}I_z^{j} - \\mathbf{I}^{i} \\cdot \\mathbf{I}^{j}\\right)\n",
    "$$\n",
    "\n",
    "The target unitary transformation is a simple $\\pi/2$-pulse about the x-axis\n",
    "$$\n",
    "U_\\text{target} = \\exp\\left(-i \\frac{\\pi}{4} \\sum_j I_x^j \\right)\n",
    "$$\n",
    "\n",
    "<!-- Hamiltonian is set to be the 0th-order average Hamiltonian from the WHH-4 pulse sequence, which is designed to remove the dipolar interaction term from the internal Hamiltonian. The pulse sequence is $\\tau, \\overline{X}, \\tau, Y, \\tau, \\tau, \\overline{Y}, \\tau, X, \\tau$.\n",
    "The zeroth-order average Hamiltonian for the WAHUHA pulse sequence is\n",
    "$$\n",
    "H_\\text{WHH}^{(0)} = \\delta / 3 \\sum_i^N \\left( I_x^{i} + I_y^{i} + I_z^{i} \\right)\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3  # 4-spin system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemical_shifts = np.random.normal(scale=50, size=(N,))\n",
    "Hcs = sum(\n",
    "    [qt.tensor(\n",
    "        [qt.identity(2)]*i\n",
    "        + [chemical_shifts[i] * qt.sigmaz()]\n",
    "        + [qt.identity(2)]*(N-i-1)\n",
    "    ) for i in range(N)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dipolar_matrix = np.random.normal(scale=50, size=(N, N))\n",
    "Hdip = sum([\n",
    "    dipolar_matrix[i, j] * (\n",
    "        2 * qt.tensor(\n",
    "            [qt.identity(2)]*i\n",
    "            + [qt.sigmaz()]\n",
    "            + [qt.identity(2)]*(j-i-1)\n",
    "            + [qt.sigmaz()]\n",
    "            + [qt.identity(2)]*(N-j-1)\n",
    "        )\n",
    "        - qt.tensor(\n",
    "            [qt.identity(2)]*i\n",
    "            + [qt.sigmax()]\n",
    "            + [qt.identity(2)]*(j-i-1)\n",
    "            + [qt.sigmax()]\n",
    "            + [qt.identity(2)]*(N-j-1)\n",
    "        )\n",
    "        - qt.tensor(\n",
    "            [qt.identity(2)]*i\n",
    "            + [qt.sigmay()]\n",
    "            + [qt.identity(2)]*(j-i-1)\n",
    "            + [qt.sigmay()]\n",
    "            + [qt.identity(2)]*(N-j-1)\n",
    "        )\n",
    "    )\n",
    "    for i in range(N) for j in range(i+1, N)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hsys = Hcs + Hdip\n",
    "X = qt.tensor([qt.sigmax()]*N)\n",
    "Y = qt.tensor([qt.sigmay()]*N)\n",
    "# Z = qt.tensor([qt.sigmaz()]*N)\n",
    "Hcontrols = [50e3 * X, 50e3 * Y]\n",
    "target = qt.propagator(X, np.pi/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SpinSystemContinuousEnv` simulates the quantum system given above, and exposes relevant methods for RL (including a `step` method that takes an action and returns an observation and reward, a `reset` method to reset the system).\n",
    "\n",
    "**TODO**: rewrite `spin_system_continuous` so it only uses `tensorflow` (no `tf-agents`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = spin_system_continuous.SpinSystemContinuousEnv(\n",
    "    Hsys=Hsys,\n",
    "    Hcontrols=Hcontrols,\n",
    "    target=target,\n",
    "    discount_factor=gamma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define actor and critic networks\n",
    "\n",
    "The observations of the system are sequences of control amplitudes that have been performed on the system (which most closely represents the knowledge of a typical experimental system). Both the actor and the critic (value) networks share an LSTM layer to convert the sequence of control amplitudes to a hidden state, and two dense layers. Separate policy and value \"heads\" are used for the two different networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.layers.LSTM(64)\n",
    "stateful_lstm = tf.keras.layers.LSTM(64, stateful=True)\n",
    "hidden1 = tf.keras.layers.Dense(64, activation=tf.keras.activations.relu)\n",
    "hidden2 = tf.keras.layers.Dense(64, activation=tf.keras.activations.relu)\n",
    "policy = tf.keras.layers.Dense(2, activation=tf.keras.activations.tanh)\n",
    "value = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = tf.keras.models.Sequential([\n",
    "    lstm,\n",
    "    hidden1,\n",
    "    hidden2,\n",
    "    policy\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net = tf.keras.models.Sequential([\n",
    "    lstm,\n",
    "    hidden1,\n",
    "    hidden2,\n",
    "    value\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_actor_net = tf.keras.models.Sequential([\n",
    "    stateful_lstm,\n",
    "    hidden1,\n",
    "    hidden2,\n",
    "    policy\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define PPO agent\n",
    "\n",
    "[Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) is a state-of-the-art RL algorithm that can be used for both discrete and continuous action spaces. PPO prevents the policy from over-adjusting during training by defining a clipped policy gradient loss function:\n",
    "$$\n",
    "L^\\text{clip}(\\theta) = \\mathbb{E}_t\\left[\n",
    "\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(\n",
    "    r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\n",
    ")\\hat{A}_t\n",
    "\\right]\n",
    "$$\n",
    "where the \"importance ratio\" $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_\\text{old}}(a_t|s_t)}$ is the relative probability of choosing the action under the new policy compared to the old policy. By clipping the loss function, there is non-zero gradient only in a small region around the original policy.\n",
    "\n",
    "Because the actor and critic networks share layers, the total loss function is used for training\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_t \\left[\n",
    "-L^\\text{clip}(\\theta) + c_1 L^\\text{VF}(\\theta)\n",
    "\\right]\n",
    "$$\n",
    "with $L^\\text{VF}(\\theta)$ as the MSE loss for value estimates.\n",
    "\n",
    "Basing off TF-Agents [abstract base class](https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/TFAgent). Also using [PPOAgent code](https://github.com/tensorflow/agents/blob/v0.6.0/tf_agents/agents/ppo/ppo_agent.py#L746)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect some experience from the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following collects experience by interacting with the environment. Right now it's really slow, so I'm using `%%time` to time the code and `%%prun` to profile the code and see where the slow-downs are happening.\n",
    "\n",
    "It runs 100 timesteps in 3.5s. 1000 timesteps in 86s. Gets much slower the longer you run it.\n",
    "\n",
    "- [ ] rewrite actor/critic so they can take lstm state as input and return final lstm state as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data should have dimensions `batch_size * [other dims]`, shouldn't just be `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, step_types, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        rewards: A tensor of rewards for the episode. Should have size batch_size * 1.\n",
    "        step_types: A tensor of step types, 1 if a normal step.\n",
    "    \"\"\"\n",
    "    returns = [0] * rewards.shape[0]\n",
    "    returns[-1] = rewards[-1]\n",
    "    for i in range(1, len(rewards)):\n",
    "        returns[-(i + 1)] = gamma * returns[-i] * tf.cast(step_types[-(i + 1)] == 1, tf.float32) + rewards[-(i+1)]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_and_mask(obs, max_sequence_length=500):\n",
    "    obs2 = []\n",
    "    mask = []\n",
    "    num_features = obs[0].shape[-1]\n",
    "    for i in range(len(obs)):\n",
    "        obs_length = obs[i].shape[-2]\n",
    "        obs2.append(tf.concat(\n",
    "            [tf.cast(obs[i], tf.float32), tf.zeros((1, max_sequence_length - obs_length, num_features))],\n",
    "            axis=1\n",
    "        ))\n",
    "        mask.append(tf.concat(\n",
    "            [tf.ones((1, obs_length)), tf.zeros((1, max_sequence_length - obs_length))],\n",
    "            axis=1\n",
    "        ))\n",
    "    obs2 = tf.squeeze(tf.stack(obs2))\n",
    "    mask = tf.squeeze(tf.stack(mask))\n",
    "    return obs2, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(num_steps=500, stddev=1e-3, max_sequence_length=500):\n",
    "    step_types = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    action_means = []\n",
    "    rewards = []\n",
    "    step = train_env.reset()\n",
    "    if stateful_actor_net.built:\n",
    "        stateful_actor_net.reset_states()\n",
    "    # collect experience\n",
    "    for _ in range(num_steps):\n",
    "        observations.append(step.observation)\n",
    "        action_mean = stateful_actor_net(tf.expand_dims(step.observation[:, -1, :], 1)) #collect_action(step.observation, stddev=stddev)\n",
    "        action = action_mean + tf.random.normal(shape=action_mean.shape, stddev=stddev)\n",
    "        actions.append(action)\n",
    "        action_means.append(action_mean)\n",
    "        step = train_env.step(action)\n",
    "        rewards.append(step.reward)\n",
    "        step_types.append(step.step_type)\n",
    "        if step.step_type == 2:\n",
    "            stateful_actor_net.reset_states()\n",
    "            step = train_env.reset()\n",
    "    # put data into tensors\n",
    "    step_types = tf.stack(step_types)\n",
    "    actions = tf.squeeze(tf.stack(actions))\n",
    "    action_means = tf.squeeze(tf.stack(action_means))\n",
    "    rewards = tf.stack(rewards)\n",
    "    # reshape observations to be same sequence length, and create\n",
    "    # a mask for original sequence length\n",
    "    obs, mask = get_obs_and_mask(observations, max_sequence_length)\n",
    "    returns = tf.stack(calculate_returns(rewards, step_types))\n",
    "    advantages = returns - critic_net(obs, mask=mask)\n",
    "    # TODO check below... I think this should be a 500x1 tensor\n",
    "    old_action_log_probs = tf.reduce_sum(-(actions - action_means)**2 / stddev**2, axis=1, keepdims=True)\n",
    "    return (obs, mask, actions, action_means,\n",
    "            rewards, step_types, returns,\n",
    "            advantages, old_action_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f f f()\n",
    "(obs, mask, actions,\n",
    " action_means, rewards, step_types, returns,\n",
    " advantages, old_action_log_probs) = collect_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "mse = tf.losses.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not actor_net.built:\n",
    "    actor_net.build(input_shape=(None, None, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of trainable variables that should be updated when minimizing the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_vars = critic_net.trainable_variables\n",
    "actor_vars = actor_net.trainable_variables\n",
    "trainable_variables = set()\n",
    "for var in critic_vars + actor_vars:\n",
    "    trainable_variables.add(var.ref())\n",
    "trainable_variables = list(trainable_variables)\n",
    "trainable_variables = [var.deref() for var in trainable_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(\n",
    "        actor_net,\n",
    "        critic_net,\n",
    "        trainable_variables,\n",
    "        obs,\n",
    "        mask,\n",
    "        actions,\n",
    "        action_means,\n",
    "        old_action_log_probs,\n",
    "        returns,\n",
    "        advantages,\n",
    "        epsilon=.2,\n",
    "        c1=1):\n",
    "    \"\"\"\n",
    "    Returns: tuple containing\n",
    "        l: Total loss.\n",
    "        grad: Gradient of loss wrt trainable variables.\n",
    "    \"\"\"\n",
    "    advantages = tf.expand_dims(advantages, -1)\n",
    "    batch_size = obs.shape[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_log_probs = tf.reduce_sum(-(actions - actor_net(obs, mask))**2 / stddev**2,\n",
    "                                         axis=1,\n",
    "                                         keepdims=True)\n",
    "        importance_ratio = tf.exp(action_log_probs - old_action_log_probs)\n",
    "        loss_clip = tf.reduce_sum(tf.minimum(\n",
    "            importance_ratio * advantages,\n",
    "            tf.clip_by_value(\n",
    "                importance_ratio,\n",
    "                1 - epsilon,\n",
    "                1 + epsilon) * advantages\n",
    "        )) / batch_size\n",
    "        loss_value = mse(tf.squeeze(returns), tf.squeeze(critic_net(obs, mask)))\n",
    "        l = -loss_clip + c1 * loss_value\n",
    "        return tape.gradient(l, trainable_variables), loss_clip, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch(obs, mask, actions, action_means, old_action_log_probs, returns, advantages,\n",
    "                    actor_net, critic_net, trainable_variables,\n",
    "                    num_epochs=10, minibatch_size=100):\n",
    "    for i in range(num_epochs):\n",
    "        minibatch = np.random.choice(obs.shape[0], size=minibatch_size)\n",
    "        grads, loss_clip, loss_value = grad(\n",
    "            actor_net, critic_net, trainable_variables,\n",
    "            tf.gather(obs, indices=minibatch),\n",
    "            tf.gather(mask, indices=minibatch),\n",
    "            tf.gather(actions, indices=minibatch),\n",
    "            tf.gather(action_means, indices=minibatch),\n",
    "            tf.gather(old_action_log_probs, indices=minibatch),\n",
    "            tf.gather(returns, indices=minibatch),\n",
    "            tf.gather(advantages, indices=minibatch))\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        print(f'{i}:\\tloss_clip: {loss_clip}\\tloss_value: {loss_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_minibatch(obs, mask, actions, action_means, old_action_log_probs, returns, advantages,\n",
    "                actor_net, critic_net, trainable_variables,\n",
    "                num_epochs=10, minibatch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_actor_net.layers[0].set_weights(actor_net.layers[0].get_weights())\n",
    "stateful_actor_net.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    \"\"\"Collect experience, train networks\n",
    "    \"\"\"\n",
    "    (obs, mask, actions,\n",
    "     action_means, rewards, step_types, returns,\n",
    "     advantages, old_action_log_probs) = collect_data()\n",
    "    print('collected data')\n",
    "    # train\n",
    "    train_minibatch(obs, mask, actions, action_means,\n",
    "                    old_action_log_probs, returns, advantages,\n",
    "                    actor_net, critic_net, trainable_variables,\n",
    "                    num_epochs=10, minibatch_size=64)\n",
    "    print('trained networks')\n",
    "    stateful_actor_net.layers[0].set_weights(actor_net.layers[0].get_weights())\n",
    "    stateful_actor_net.reset_states()\n",
    "    print('reset state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(obs, mask, actions,\n",
    " action_means, rewards, step_types, returns,\n",
    " advantages, old_action_log_probs) = collect_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_actor_net.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
