{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design using DQN\n",
    "_Written by Will Kaufman_\n",
    "\n",
    "This notebook walks through a reinforcement learning approach to pulse sequence design for spin systems. [TF-Agents](https://www.tensorflow.org/agents) is used as a reinforcement learning library that uses Tensorflow, a common machine learning framework.\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Figure out if an RNN starts with empty initial state (even if start of trajectory isn't initial state), or if it starts with initial state saved in replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import spin_simulation as ss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network, q_rnn_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from environments import spin_sys_discrete_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(spin_sys_discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define algorithm hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "episode_length = 5 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = episode_length  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 12 #64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 5  # @param {type:\"integer\"}\n",
    "eval_interval = 200  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the spin system\n",
    "\n",
    "This sets the parameters of the system ($N$ spin-1/2 particles, which corresponds to a Hilbert space with dimension $2^N$). For the purposes of simulation, $\\hbar \\equiv 1$.\n",
    "\n",
    "The total internal Hamiltonian is given by\n",
    "$$\n",
    "H_\\text{int} = C H_\\text{dip} + \\delta \\sum_i^N I_z^{i}\n",
    "$$\n",
    "where $C$ is the coupling strength, $\\delta$ is the chemical shift strength (each spin is assumed to be identical), and $H_\\text{dip}$ is given by\n",
    "$$\n",
    "H_\\text{dip} = \\sum_{i,j}^N d_{i,j} \\left(3I_z^{i}I_z^{j} - \\mathbf{I}^{i} \\cdot \\mathbf{I}^{j}\\right)\n",
    "$$\n",
    "\n",
    "The target Hamiltonian is set to be the 0th-order average Hamiltonian from the WHH-4 pulse sequence, which is designed to remove the dipolar interaction term from the internal Hamiltonian. The pulse sequence is $\\tau, \\overline{X}, \\tau, Y, \\tau, \\tau, \\overline{Y}, \\tau, X, \\tau$.\n",
    "The zeroth-order average Hamiltonian for the WAHUHA pulse sequence is\n",
    "$$\n",
    "H_\\text{WHH}^{(0)} = \\delta / 3 \\sum_i^N \\left( I_x^{i} + I_y^{i} + I_z^{i} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=4\n",
    "dim = 2**N\n",
    "coupling = 1e3\n",
    "delta = 500\n",
    "(X,Y,Z) = ss.get_total_spin(N=N, dim=dim)\n",
    "H_target = ss.get_H_WHH_0(X, Y, Z, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SpinSystemDiscreteEnv` class keeps track of the system dynamics, and implements methods that are necessary for RL:\n",
    "\n",
    "- `action_spec`: Returns an `ArraySpec` that gives the shape and range of a valid action. For example, in a discrete action space, an action will be an integer scalar between 0 and `numActions - 1`. For a continuous action space, an action will be a 3-dimensional vector representing phase, amplitude, and duration of the pulse.\n",
    "- `observation_spec`: Returns an `ArraySpec` that gives the shape and range of a valid observation. In this case, the observations are all the actions performed on the environment so far.\n",
    "- `_reset`: Resets the environment. This means setting the propagator to the identity, and choosing a new random dipolar interaction matrix $(d_{i,j})$.\n",
    "- `_step`: Evolves the environment according to the action. Returns a `TimeStep` which includes the step type (`FIRST`, `MID`, or `LAST`), the **reward**, the discount rate to apply to future rewards, and an **observation** of the environment.\n",
    "\n",
    "The reward function $r(s,a)$ can in general depend on the environment state _and_ action performed. However, because the goal of pulse sequence design is to find high-fidelity pulse sequences, the reward only depends on the state. \n",
    "$$\n",
    "r = -\\log \\left( 1-\n",
    "    \\left|\n",
    "        \\frac{\\text{Tr} (U_\\text{target}^\\dagger U_\\text{exp})}{\\text{Tr}(\\mathbb{1})}\n",
    "    \\right|\n",
    "    \\right)\n",
    "% = -\\log\\left( 1- \\text{fidelity}(U_\\text{target}, U_\\text{exp}) \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = spin_sys_discrete_sequence.SpinSystemDiscreteSequenceEnv(N=4, dim=16, coupling=1e3,\n",
    "    delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "    delay_after=True, state_size=episode_length)\n",
    "# env.reset()\n",
    "\n",
    "# train_py_env = spin_sys_discrete.SpinSystemDiscreteEnv(N=4, dim=16, coupling=1e3,\n",
    "#     delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "#     delay_after=True)\n",
    "# eval_py_env = spin_sys_discrete.SpinSystemDiscreteEnv(N=4, dim=16, coupling=1e3,\n",
    "#     delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "#     delay_after=True)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "\n",
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)\n",
    "\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Q-network\n",
    "\n",
    "For the DQN algorithm, a Q-network must be defined. The Q-function $Q^\\pi(s,a)$ approximates the total return from performing action $a$ in state $s$, then following policy $\\pi$.\n",
    "<!-- $$\n",
    "Q^\\pi(s,a) = \n",
    "$$ -->\n",
    "\n",
    "Because the system's state is entirely determined by the sequence of actions performed, a RNN is used for the Q-network. This means that the network has an internal state that acts as a memory of the episode. Each observation passed to the Q-network updates the internal state, and the internal state is reset at the end of every episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_net = q_network.QNetwork(\n",
    "#     train_env.observation_spec(),\n",
    "#     train_env.action_spec()\n",
    "# )\n",
    "\n",
    "# target_q_net = q_network.QNetwork(\n",
    "#     train_env.observation_spec(),\n",
    "#     train_env.action_spec()\n",
    "# )\n",
    "\n",
    "# #q_net.summary()\n",
    "# q_net(train_env.current_time_step().observation)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_rnn_net = q_rnn_network.QRnnNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec()\n",
    ")\n",
    "\n",
    "target_q_rnn_net = q_rnn_network.QRnnNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec()\n",
    ")\n",
    "\n",
    "q_rnn_net(train_env.current_time_step().observation,\n",
    "          step_type=np.array((ts.StepType.FIRST,)))[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent\n",
    "\n",
    "In RL, the \"agent\" has a policy that determines its behavior. For DQN, the agent will act greedily during evaluation (i.e. it picks the action with the maximal Q-value) and epsilon-greedily during data collection. These policies are accessed with `agent.policy` (for evaluation) and `agent.collect_policy` (for data collection).\n",
    "\n",
    "According to [the docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/tf_agent/TFAgent?hl=fa#args), I can adjust `train_sequence_length=None` for RNN-based agents. When using non-RNN DQN, though, I don't have that option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_rnn_net,\n",
    "    target_q_network=target_q_rnn_net,\n",
    "    target_update_period=10,\n",
    "    optimizer=optimizer,\n",
    "    gamma=0.99,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10, print_actions=False):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        policy_state = policy.get_initial_state(environment.batch_size)\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step, policy_state = policy_state)\n",
    "            policy_state = action_step.state\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            if print_actions:\n",
    "                print(f\"action: {action_step.action}, reward: {time_step.reward}, return: {episode_return}\")\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO include other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the replay buffer\n",
    "\n",
    "A replay buffer stores trajectories (sequences of states and actions) from data collection, and then samples those trajectories to train the agent. This increases data-efficiency and decreases bias.\n",
    "\n",
    "Trying to use the [EpisodicReplaybuffer](https://github.com/tensorflow/agents/blob/v0.5.0/tf_agents/replay_buffers/episodic_replay_buffer.py#L100) to return full episodes from the buffer. This is important when using a Q-RNN network, because the internal state must update starting from the beginning of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "#     data_spec=agent.collect_data_spec,\n",
    "#     batch_size=train_env.batch_size,\n",
    "#     max_length=replay_buffer_max_length)\n",
    "\n",
    "replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    capacity=int(replay_buffer_max_length/episode_length),\n",
    ")\n",
    "\n",
    "episode_id = tf.constant([0], dtype=\"int64\")\n",
    "\n",
    "replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, policy_state, buffer, episode_id):\n",
    "    time_step = environment.current_time_step()\n",
    "    if time_step.is_last():\n",
    "        time_step = environment.reset()\n",
    "    action_step = policy.action(time_step, policy_state)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    \n",
    "    # Add trajectory to the replay buffer\n",
    "    new_id = buffer.add_batch(traj, episode_id)\n",
    "    \n",
    "    return action_step.state, new_id\n",
    "\n",
    "def collect_data(env, policy, buffer, steps, episode_id, policy_state = None):\n",
    "    if policy_state is None:\n",
    "        policy_state = policy.get_initial_state(env.batch_size)\n",
    "    for _ in range(steps):\n",
    "        policy_state, new_id = collect_step(env, policy, policy_state, buffer, episode_id)\n",
    "        episode_id = new_id\n",
    "    return episode_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect 64 episodes from a random policy and store to the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_env.reset()\n",
    "\n",
    "episode_id = collect_data(env=train_env,\n",
    "             policy=random_policy, \n",
    "             buffer=replay_buffer,\n",
    "             steps=episode_length*32,\n",
    "             episode_id=episode_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensorflow `Dataset` takes care of sampling the replay buffer and generating trajectories quite nicely. The replay buffer can be converted to a `Dataset` which is then used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=2,\n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=episode_length).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the driver\n",
    "\n",
    "TODO\n",
    "\n",
    "- figure out best way to add replay buffer to observer\n",
    "- add writeup to this section\n",
    "- actually address algorithm performance (IT'S NOT WORKING!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_batch(trajectory):\n",
    "    \"\"\"Add a trajectory to the episodic replay buffer and update the episode id\"\"\"\n",
    "    global episode_id\n",
    "    new_id = replay_buffer.add_batch(trajectory, episode_id)\n",
    "    episode_id = new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "avg_return = tf_metrics.AverageReturnMetric()\n",
    "\n",
    "observers = [num_episodes,\n",
    "             avg_return,\n",
    "             add_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env,\n",
    "    collect_policy,\n",
    "    observers,\n",
    "    num_steps=episode_length*1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time_step, policy_state = driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(size=(4,4)) + 1j * np.random.normal(size=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.stack([a.real, a.imag], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "agent.collect_policy.action = common.function(agent.collect_policy.action)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Network is just flattening input, not taking advantage of sequential structure (need LSTM...). Create custom Q network.\n",
    "- Try out PPO..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(episode_id):\n",
    "    train_env.reset()\n",
    "    policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        final_time_step, policy_state = driver.run()\n",
    "#         for _ in range(collect_steps_per_iteration):\n",
    "#             #print(policy_state)\n",
    "#             policy_state, new_id = collect_step(train_env,\n",
    "#                                                 agent.collect_policy,\n",
    "#                                                 policy_state,\n",
    "#                                                 replay_buffer,\n",
    "#                                                 episode_id)\n",
    "#             episode_id = new_id\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience).loss\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            # print(q_net(np.zeros((1,5,5), dtype=\"float32\"))[0].numpy())\n",
    "            print(f'step = {step}: loss = {train_loss}')\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_env, agent.policy)\n",
    "            print(f'step = {step}: Average Return = {avg_return}')\n",
    "            if avg_return > 50:\n",
    "                break\n",
    "            returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f collect_step train_agent(episode_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the agent\n",
    "\n",
    "See what pulse sequences it's performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_avg_return(eval_env, agent.policy, num_episodes=1, print_actions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Q-network structure (including the encoding network, LSTM, and final dense layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_rnn_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = q_rnn_net.get_layer(\"EncodingNetwork\").get_weights()\n",
    "for weight in w:\n",
    "    print(weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what the Q-function returns for a play-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = train_env.reset()\n",
    "print(q_rnn_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(1)\n",
    "print(q_rnn_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(2)\n",
    "print(q_rnn_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(4)\n",
    "print(q_rnn_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(3)\n",
    "print(q_rnn_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(0)\n",
    "print(q_rnn_net(ts.observation, step_type=ts.step_type)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually interact with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.reset()\n",
    "# run the WHH-4 sequence\n",
    "eval_env.step(1)\n",
    "eval_env.step(2)\n",
    "eval_env.step(4)\n",
    "eval_env.step(3)\n",
    "eval_env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
