{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero supervised learning\n",
    "_Will Kaufman, January 2021_\n",
    "\n",
    "This is simply to generate training data and do supervised learning so I can test different neural network designs, and to make sure it can actually learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qutip as qt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "import pulse_sequences as ps\n",
    "import alpha_zero as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pulse_sequences' from '/Users/willkaufman/Projects/rl_pulse/rl_pulse/pulse_sequences.py'>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importlib.reload(az)\n",
    "# importlib.reload(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2  # 32\n",
    "num_collect_initial = 2  # 5000\n",
    "\n",
    "max_sequence_length = 48\n",
    "batch_size = 2048\n",
    "num_epochs = 10\n",
    "num_batches = 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the spin system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = 1e-2  # time is relative to chemical shift strength\n",
    "pulse_width = 1e-3\n",
    "N = 3  # number of spins\n",
    "ensemble_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utarget = qt.tensor([qt.identity(2)] * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = az.ReplayBuffer(int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data_no_net(x):\n",
    "#     print(f'collecting data without network ({x})')\n",
    "    config = az.Config()\n",
    "    ps_config = ps.PulseSequenceConfig(N=N, ensemble_size=ensemble_size,\n",
    "                                       max_sequence_length=max_sequence_length,\n",
    "                                       Utarget=Utarget,\n",
    "                                       pulse_width=pulse_width, delay=delay)\n",
    "    return az.make_sequence(config, ps_config, network=None, rng=ps_config.rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(num_cores) as pool:\n",
    "    output = pool.map(collect_data_no_net, range(num_collect_initial))\n",
    "for stat in output:\n",
    "    az.add_stats_to_buffer(stat, rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, probabilities, values = zip(*rb.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_states = az.pad_and_pack(states)\n",
    "probabilities = torch.cat(probabilities).view(len(rb), -1)\n",
    "values = torch.cat(values).view(len(rb), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(packed_states, 'states.pt')\n",
    "torch.save(probabilities, 'probabilities.pt')\n",
    "torch.save(values, 'values.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = az.Policy()\n",
    "value = az.Value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_states = torch.load('../../data/2021-01/20210116-110000/states.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_states = torch.nn.utils.rnn.pad_packed_sequence(packed_states, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = torch.load('../../data/2021-01/20210116-110000/probabilities.pt')\n",
    "values = torch.load('../../data/2021-01/20210116-110000/values.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulseSequenceDataset(Dataset):\n",
    "    def __init__(self, padded_states, probabilities, values):\n",
    "        super().__init__()\n",
    "        self.padded_states = padded_states\n",
    "        self.probabilities = probabilities\n",
    "        self.values = values\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.padded_states[0][idx, ...],\n",
    "                self.padded_states[1][idx, ...],\n",
    "                self.probabilities[idx, ...],\n",
    "                self.values[idx, ...])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.values.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PulseSequenceDataset(padded_states, probabilities, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    states, lengths, probs, vals = zip(*data)\n",
    "    states = torch.stack(states)\n",
    "    lengths = torch.stack(lengths)\n",
    "    packed_states = torch.nn.utils.rnn.pack_padded_sequence(states, lengths,\n",
    "                                                            batch_first=True,\n",
    "                                                            enforce_sorted=False)\n",
    "    probs = torch.stack(probs)\n",
    "    vals = torch.stack(vals)\n",
    "    return packed_states, probs, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_outputs, _ = policy(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = values.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1308.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0., 28320., 34896.,\n",
       "         3552.,     0.,    48.,   528.,   480.,   624.,   720.,  1248.,\n",
       "         1536.,  1728.,  2256.,  1680.,  2112.,  1104.,  2160.,  1632.,\n",
       "         1296.,  1776.,  1152.,  1104.,   528.,   624.,   432.,   672.,\n",
       "          480.,   624.,   384.,   480.,   192.,    96.,     0.,    48.,\n",
       "           48.,    96.]),\n",
       " array([-1.        , -0.92981917, -0.85963833, -0.78945744, -0.7192766 ,\n",
       "        -0.6490958 , -0.57891494, -0.5087341 , -0.43855324, -0.36837238,\n",
       "        -0.29819155, -0.2280107 , -0.15782985, -0.08764901, -0.01746817,\n",
       "         0.05271268,  0.12289353,  0.19307438,  0.2632552 ,  0.33343607,\n",
       "         0.4036169 ,  0.47379774,  0.5439786 ,  0.61415946,  0.6843403 ,\n",
       "         0.75452113,  0.82470196,  0.8948828 ,  0.9650637 ,  1.0352445 ,\n",
       "         1.1054254 ,  1.1756063 ,  1.245787  ,  1.3159679 ,  1.3861487 ,\n",
       "         1.4563296 ,  1.5265105 ,  1.5966913 ,  1.6668721 ,  1.7370529 ,\n",
       "         1.8072338 ,  1.8774147 ,  1.9475955 ,  2.0177763 ,  2.0879571 ,\n",
       "         2.158138  ,  2.228319  ,  2.2984998 ,  2.3686805 ,  2.4388614 ,\n",
       "         2.5090423 ], dtype=float32),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vals, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2906089783668876"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(vals > .25) / vals.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36310405\n",
      "0.31326103\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(vals))\n",
    "print(np.var(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1366603\n",
      "0.18059035\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(vals[vals > .2]))\n",
    "print(np.var(vals[vals > .2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify entropy in MCTS probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = -torch.nansum(probabilities * torch.log(probabilities), 1)\n",
    "entropies = entropies.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95964"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropies.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4354445417031387"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(entropies <= np.log(4)) / entropies.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x7fa581914f10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(entropies, bins=50)\n",
    "plt.vlines(x=np.log(2), ymin=0, ymax=20000, label='2')\n",
    "plt.vlines(x=np.log(3), ymin=0, ymax=20000, label='3')\n",
    "plt.vlines(x=np.log(4), ymin=0, ymax=20000, label='4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize networks with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size=6, lstm_size=64, output_size=5):\n",
    "        super(Policy, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=lstm_size,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_size, output_size)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        \"\"\"Calculates the policy from state x\n",
    "\n",
    "        Args:\n",
    "            x: The state of the pulse sequence. Either a tensor with\n",
    "                shape B*T*(num_actions + 1), or a packed sequence of states.\n",
    "        \"\"\"\n",
    "        if h0 is None or c0 is None:\n",
    "            x, (h, c) = self.lstm(x)\n",
    "        else:\n",
    "            x, (h, c) = self.lstm(x, (h0, c0))\n",
    "        if type(x) is torch.Tensor:\n",
    "            x = x[:, -1, :]\n",
    "        elif type(x) is nn.utils.rnn.PackedSequence:\n",
    "            # x is PackedSequence, need to get last timestep from each\n",
    "            x, lengths = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "            idx = (\n",
    "                lengths.long() - 1\n",
    "            ).view(\n",
    "                -1, 1\n",
    "            ).expand(\n",
    "                len(lengths), x.size(2)\n",
    "            ).unsqueeze(1)\n",
    "            x = x.gather(1, idx).squeeze(1)\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "        return x, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, input_size=6, lstm_size=64):\n",
    "        super(Value, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=lstm_size,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_size, 1)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        \"\"\"Calculates the value from state x\n",
    "\n",
    "        Args:\n",
    "            x: The state of the pulse sequence. Either a tensor with\n",
    "                shape B*T*(num_actions + 1), or a packed sequence of states.\n",
    "        \"\"\"\n",
    "        if h0 is None or c0 is None:\n",
    "            x, (h, c) = self.lstm(x)\n",
    "        else:\n",
    "            x, (h, c) = self.lstm(x, (h0, c0))\n",
    "        if type(x) is torch.Tensor:\n",
    "            x = x[:, -1, :]\n",
    "        elif type(x) is nn.utils.rnn.PackedSequence:\n",
    "            # x is PackedSequence, need to get last timestep from each\n",
    "            x, lengths = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "            idx = (\n",
    "                lengths.long() - 1\n",
    "            ).view(\n",
    "                -1, 1\n",
    "            ).expand(\n",
    "                len(lengths), x.size(2)\n",
    "            ).unsqueeze(1)\n",
    "            x = x.gather(1, idx).squeeze(1)\n",
    "        x = self.fc1(x)\n",
    "        return x, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(lstm_size=32)\n",
    "value = Value(lstm_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optim.Adam(policy.parameters())\n",
    "value_optimizer = optim.Adam(value.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO keep trying out different learning rates, see if that' enough to start learning\n",
    "then move on to different NN architectures, continue from there......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_policy_loss = torch.as_tensor(100)\n",
    "# old_value_loss = torch.as_tensor(100)\n",
    "# for epoch in range(num_epochs):\n",
    "#     batch_num = 0\n",
    "#     for packed_states, probs, vals in iter(loader):\n",
    "#         print(f'on epoch {epoch}, batch {batch_num}', end='\\t')\n",
    "#         policy_optimizer.zero_grad()\n",
    "#         value_optimizer.zero_grad()\n",
    "#         policy_output, _ = policy(packed_states)\n",
    "#         value_outputs, _ = value(packed_states)\n",
    "#         policy_loss = -1 / batch_size * torch.sum(sample[1] * torch.log(policy_output))\n",
    "#         value_loss = F.mse_loss(value_outputs, sample[2])\n",
    "#         policy_loss.backward()\n",
    "#         policy_optimizer.step()\n",
    "#         value_loss.backward()\n",
    "#         value_optimizer.step()\n",
    "#         print(f'policy loss: {policy_loss}, value loss: {value_loss}')\n",
    "#         if old_policy_loss / policy_loss < 1.01:\n",
    "#             print(f'policy loss is less than 1% better ({old_policy_loss / policy_loss})')\n",
    "#         if old_value_loss / value_loss < 1.01:\n",
    "#             print(f'value loss is less than 1% better ({old_value_loss / value_loss})')\n",
    "#         old_policy_loss = policy_loss\n",
    "#         old_value_loss = value_loss\n",
    "#         batch_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some commentary... Seems like the training process flatlines with cross entropy loss around 1.6 and MSE loss around 0.3. Things to play around with...\n",
    "\n",
    "- changing network architecture\n",
    "- changing learning rate etc.\n",
    "\n",
    "LSTM hidden size 64 stagnated at policy loss: 1.606, value loss: 0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class: combining policy and value networks\n",
    "\n",
    "TODO add more residual layers (maybe??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"A network with policy and value heads\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size=6,\n",
    "                 rnn_size=64,\n",
    "                 fc_size=32,\n",
    "                 policy_output_size=5,\n",
    "                 value_output_size=1):\n",
    "        super(Network, self).__init__()\n",
    "        # define layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=rnn_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "        )\n",
    "        self.batchnorm1 = nn.BatchNorm1d(rnn_size)\n",
    "        self.fc1 = nn.Linear(rnn_size, fc_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(fc_size)\n",
    "        self.fc2 = nn.Linear(fc_size, fc_size)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(fc_size)\n",
    "        self.policy = nn.Linear(fc_size, policy_output_size)\n",
    "        self.value = nn.Linear(fc_size, value_output_size)\n",
    "\n",
    "    def forward(self, x, h_0=None):\n",
    "        \"\"\"Calculates the policy and value from state x\n",
    "\n",
    "        Args:\n",
    "            x: The state of the pulse sequence. Either a tensor with\n",
    "                shape B*T*(num_actions + 1), or a packed sequence of states.\n",
    "        \"\"\"\n",
    "        # RNN layer\n",
    "        if h_0 is None:\n",
    "            x, h = self.gru(x)\n",
    "        else:\n",
    "            x, h = self.gru(x, h_0)\n",
    "        if type(x) is torch.Tensor:\n",
    "            x = x[:, -1, :]\n",
    "        elif type(x) is nn.utils.rnn.PackedSequence:\n",
    "            # x is PackedSequence, need to get last timestep from each\n",
    "            x, lengths = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "            idx = (\n",
    "                lengths.long() - 1\n",
    "            ).view(-1, 1).expand(\n",
    "                len(lengths), x.size(2)\n",
    "            ).unsqueeze(1)\n",
    "            x = x.gather(1, idx).squeeze(1)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        # hidden residual layers\n",
    "        x = F.relu(self.batchnorm2(self.fc1(x)))\n",
    "        x = F.relu(self.batchnorm3(self.fc2(x)) + x)  # skip connection from '+ x'\n",
    "        policy = F.softmax(self.policy(x), dim=1)\n",
    "        value = self.value(x)\n",
    "        return policy, value, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "# p, v, h = net(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to take about 40 minibatches of size 2048 to get to stable learning. The MSE is the same as the variance of the data though, so it's not picking up on much information..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    batch_num = 0\n",
    "    for packed_states, probs, vals in iter(loader):\n",
    "        print(f'on epoch {epoch}, batch {batch_num}', end='\\t')\n",
    "        net_optimizer.zero_grad()\n",
    "        p, v, _ = net(packed_states)\n",
    "        policy_loss = -1 / batch_size * torch.sum(probs * torch.log(p))\n",
    "        value_loss = F.mse_loss(v, vals)\n",
    "        loss = policy_loss + value_loss\n",
    "        loss.backward()\n",
    "        net_optimizer.step()\n",
    "        print(f'policy loss: {policy_loss}, value loss: {value_loss}')\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared memory\n",
    "\n",
    "I want the replay buffer and the neural network parameters to be shared between processes. I think pytorch's `torch.multiprocessing` module ([see here](https://pytorch.org/docs/stable/multiprocessing.html)) will be the best option. But there's very little documentation, they just point to the native `multiprocessing` module. Helpful...\n",
    "\n",
    "Also `multiprocessing` has a [shared memory module](https://docs.python.org/3/library/multiprocessing.shared_memory.html) that could work. But it looks pretty low-level, won't be super easy to implement.\n",
    "\n",
    "I'm reading through the `multiprocessing` module docs right now...\n",
    "\n",
    "Alright, I think what I want to do is to use `Manager()` and write a proxy class for the replay buffer. Then I instantiate a `ReplayBuffer` object with the maanger, and then each process gets a proxy that can read/write to it. See [the official docs](https://docs.python.org/3/library/multiprocessing.html#proxy-objects) and [this StackOverflow question](https://stackoverflow.com/questions/3671666/sharing-a-complex-object-between-processes) for help.\n",
    "\n",
    "Also check out [this pytorch doc](https://pytorch.org/docs/stable/notes/multiprocessing.html) for how to share a model's memory with multiple processes.\n",
    "\n",
    "# TODO\n",
    "\n",
    "- [ ] `f` is the MCTS worker process, `g` is the training process, l is the replay buffer and model is the network\n",
    "- [ ] See how `Value()` works, flesh out replay buffer functionality (write methods for it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Lock, Queue, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(num, buffer, index, model):\n",
    "    print(f'in f {num}')\n",
    "    rng = np.random.default_rng()  # to make each process use different random numbers\n",
    "    state = rng.normal(size=(5, 10, 6))\n",
    "    state = torch.tensor(state).float()  # converts to float32 tensor\n",
    "#     print(state)\n",
    "    sleep(num)\n",
    "    print('trying to calculate result')\n",
    "    with torch.no_grad():  # no_grad so no backpropagation information is calculated/stored\n",
    "        # also don't have to detach tensors then\n",
    "        output = model(state)\n",
    "    print('appending...')\n",
    "#     with index.get_lock():\n",
    "    buffer[index.value] = output\n",
    "    index.value += 1\n",
    "    # TODO wrap around index if\n",
    "\n",
    "def g(buffer, index, model):\n",
    "    sleep(1.5)\n",
    "    print('got to g')\n",
    "    for i in range(5):\n",
    "        sleep(1)\n",
    "        # print(buffer)\n",
    "        print(index)\n",
    "        # print(model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with mp.Manager() as manager:\n",
    "        buffer = manager.list([None] * buffer_size)\n",
    "        index = manager.Value(typecode='i', value=0)\n",
    "        model = Network()\n",
    "        model.share_memory()\n",
    "        q = mp.Process(target=g, args=(buffer, index, model))\n",
    "        q.start()\n",
    "        workers = []\n",
    "        for i in range(4):\n",
    "            workers.append(mp.Process(target=f, args=(i, buffer, index, model)))\n",
    "            workers[-1].start()\n",
    "        q.join()\n",
    "        for w in workers:\n",
    "            w.join()\n",
    "        print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in f 3\n",
      "in f 5\n",
      "in f 13\n",
      "in f 8\n",
      "in f 0\n",
      "in f 11in f 4\n",
      "\n",
      "in f 14\n",
      "in f 15\n",
      "in f 19\n",
      "in f 17\n",
      "in f 12\n",
      "in f 9\n",
      "in f 10\n",
      "in f 6\n",
      "in f 7\n",
      "in f 1\n",
      "in f 16\n",
      "in f 18\n",
      "in f 2\n",
      "[17, 12, 9, 10, 6, 7, 1, 16, 18, 2]\n",
      "<class 'multiprocessing.managers.ListProxy'>\n"
     ]
    }
   ],
   "source": [
    "def f(num, buffer, index, lock):\n",
    "    rng = np.random.default_rng()\n",
    "    sleep(rng.random() * 2)\n",
    "    print(f'in f {num}')\n",
    "    with lock:\n",
    "        if len(buffer) < 10:\n",
    "            buffer.append(num)\n",
    "            index.value += 1\n",
    "        else:\n",
    "            buffer[index.value] = num\n",
    "            index.value += 1\n",
    "        if index.value >= 10:\n",
    "            index.value = 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with mp.Manager() as manager:\n",
    "        buffer = manager.list()\n",
    "        index = manager.Value(typecode='i', value=0)\n",
    "        lock = manager.RLock()\n",
    "        workers = []\n",
    "        for i in range(20):\n",
    "            workers.append(mp.Process(target=f, args=(i, buffer, index, lock)))\n",
    "            workers[-1].start()\n",
    "        for w in workers:\n",
    "            w.join()\n",
    "        print(buffer)\n",
    "        print(type(buffer))\n",
    "        a = list(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out well-performing pulse sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_idx = values.squeeze() > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_full_sequence = padded_states[1] == 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_sequences = padded_states[0][candidate_idx * is_full_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0507, 2.1880, 2.0568, 2.3259, 2.0891, 2.0475, 2.0465, 2.0025, 2.0391,\n",
       "        2.4547, 2.0190, 2.0536, 2.0163, 2.4376, 2.1477, 2.0211, 2.0812, 2.0972,\n",
       "        2.0765, 2.1756, 2.5090, 2.0155, 2.1318])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[candidate_idx * is_full_sequence].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 48, 6])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_config = ps.PulseSequenceConfig(N=3, ensemble_size=10, max_sequence_length=48, Utarget=Utarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "for i in range(candidate_sequences.size(0)):\n",
    "    sequence = list(torch.argmax(candidate_sequences[i,...], dim=1).numpy()[1:] - 1)\n",
    "    sequence += ps.get_valid_time_suspension_pulses(sequence, num_pulses=5, sequence_length=48)\n",
    "    fidelity = ps.get_mean_fidelity(sequence, Utarget, ps_config.pulses_ensemble)\n",
    "    reward = -np.log10(1 - fidelity + 1e-200)\n",
    "    candidates.append((sequence, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.sort(key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2353464312128435\n",
      "2.1204414626672023\n",
      "2.0665367613756347\n",
      "2.000438338687582\n",
      "1.9058772164486646\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(candidates[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with yxx48 sequence\n",
    "yxx48_fidelity = ps.get_mean_fidelity(ps.yxx48, Utarget, ps_config.pulses_ensemble)\n",
    "yxx48_reward = -np.log10(1 - yxx48_fidelity + 1e-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7053378424600925"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yxx48_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.301763111038954"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fidelity = ps.get_mean_fidelity(ps.mcts48_5, Utarget, ps_config.pulses_ensemble)\n",
    "reward = -np.log10(1 - fidelity + 1e-200)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['\\\\tau', 'X,\\\\tau', '\\\\overline{X},\\\\tau', 'Y,\\\\tau', '\\\\overline{Y},\\\\tau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ''\n",
    "for j in range(4):\n",
    "    j = j * 12\n",
    "    string += ','.join([names[i] for i in ps.mcts48_5[j:(j+12)]]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y,2\\tau,\\overline{X},\\tau,\\overline{X},\\tau,Y,\\tau,\\overline{Y},2\\tau,Y,\\tau,\\overline{X},\\tau,Y,\\tau,\\overline{X},\\tau,Y,\\tau\n",
      "Y,\\tau,\\overline{X},\\tau,\\overline{X},\\tau,Y,\\tau,\\overline{X},\\tau,Y,\\tau,Y,\\tau,X,2\\tau,\\overline{Y},2\\tau,X,\\tau\n",
      "\\overline{Y},\\tau,Y,\\tau,Y,\\tau,Y,\\tau,Y,\\tau,X,2\\tau,Y,\\tau,\\overline{X},\\tau,Y,2\\tau,X,\\tau\n",
      "Y,2\\tau,\\tau,Y,2\\tau,Y,2\\tau,\\tau,X,2\\tau,2\\tau\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(string.replace('\\\\tau,\\\\tau', '2\\\\tau'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950084331828052"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.431798275933006"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log10(1-np.mean([.9, .99, .999]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log10(1-np.array([.9, .99, .999]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
