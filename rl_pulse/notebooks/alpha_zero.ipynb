{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero implementation for pulse sequence design\n",
    "_Will Kaufman, December 2020_\n",
    "\n",
    "[Dalgaard et. al. (2020)](https://www.nature.com/articles/s41534-019-0241-0) applied this approach to constructing shaped pulses (as I understand it), but in theory this should be as applicable to pulse sequence design, if not more so. The original [AlphaZero paper](https://science.sciencemag.org/content/362/6419/1140.full) and the [AlphaGo Zero paper](https://www.nature.com/articles/nature24270) are useful resources.\n",
    "\n",
    "The general idea behind AlphaZero (as I understand it) is to do a \"smart\" tree search that balances previous knowledge (the policy), curiosity in unexplored branches, and high-value branches. My thought is that this can be improved with AHT (i.e. knowing that by the end of the pulse sequence, the pulse sequence must be cyclic (the overall frame transformation must be identity) and there must be equal times spent on each axis). This will provide a hard constraint that will (hopefully) speed up search.\n",
    "\n",
    "## System installation\n",
    "\n",
    "Make sure the following packages are installed\n",
    "\n",
    "- `numpy`\n",
    "- `scipy`\n",
    "- `qutip`\n",
    "- `pytorch`\n",
    "- `tensorboard`\n",
    "\n",
    "## TODO\n",
    "- [ ] Collect all hyperparameters up top or in config (e.g. how many pulse sequences to collect data from)\n",
    "- [ ] Speed up LSTM (save hidden state, batch parallel pulse sequences, other?)\n",
    "- [ ] Figure out GPU utilization (if I can...)\n",
    "- [ ] Look into collecting training data and training continuously\n",
    "- [ ] Change dirichlet noise to match number of possible moves (5 for now, eventually 24)\n",
    "- [ ] Dynamically figure out how many CPUs there are available, and set pool to use that\n",
    "- [ ] Mess around with hyperparameters (e.g. in config object), see if performance improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qutip as qt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "import pulse_sequences as ps\n",
    "import alpha_zero as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pulse_sequences' from '/Users/willkaufman/Projects/rl_pulse/rl_pulse/pulse_sequences.py'>"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(az)\n",
    "importlib.reload(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2  # 32\n",
    "num_collect = 2  # 1000\n",
    "num_collect_initial = 2  # 5000\n",
    "batch_size = 64  #2048\n",
    "num_iters = 100\n",
    "\n",
    "max_sequence_length = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the spin system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = 1e-2  # time is relative to chemical shift strength\n",
    "pulse_width = 1e-3\n",
    "N = 3  # number of spins\n",
    "ensemble_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, Z = ps.get_collective_spin(N)\n",
    "# Hsys_ensemble = [ps.get_Hsys(N) for _ in range(ensemble_size)]\n",
    "# pulses_ensemble = [\n",
    "#     ps.get_pulses(H, X, Y, Z, pulse_width, delay, rot_error=0.01) for H in Hsys_ensemble\n",
    "# ]\n",
    "# Utarget = qt.identity(Hsys_ensemble[0].dims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utarget = qt.tensor([qt.identity(2)] * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smarter search with MCTS\n",
    "\n",
    "Following the [supplementary materials description under \"Search\"](https://science.sciencemag.org/content/sci/suppl/2018/12/05/362.6419.1140.DC1/aar6404-Silver-SM.pdf) to do rollouts and backpropagate information. All of the relevant code for the alpha zero algorithm is in alpha_zero.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Inspired by [this pytorch tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html). This will store data collected by MCTS rollouts. The data is the state (the pulse sequence applied so far), the empirical probability distribution based on visit counts, and the value from the end of the pulse sequence.\n",
    "\n",
    "Based on a rough idea of what was done with AlphaGo Zero and AlphaZero, I think I need to collect a lot of data and train a lot with each iteration. In AlphaGo Zero, they collected 25,000 games per iteration and trained with minibatch sizes of 2048 (training continuously I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = az.ReplayBuffer(int(1e5))  # 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of saving data in a reasonable way (and using RNN), the state is represented by a sequence, where 0 indicates the start of sequence, and 1-5 are the possible pulses (1: delay, 2: x, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill replay buffer with inital data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data_no_net(x):\n",
    "#     print(f'collecting data without network ({x})')\n",
    "    config = az.Config()\n",
    "    ps_config = ps.PulseSequenceConfig(N=N, ensemble_size=ensemble_size,\n",
    "                                       max_sequence_length=max_sequence_length,\n",
    "                                       Utarget=Utarget,\n",
    "                                       pulse_width=pulse_width, delay=delay)\n",
    "    return az.make_sequence(config, ps_config, network=None, rng=ps_config.rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(num_cores) as pool:\n",
    "    output = pool.map(collect_data_no_net, range(num_collect_initial))\n",
    "for stat in output:\n",
    "    az.add_stats_to_buffer(stat, rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS with policy and value networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = az.Policy()\n",
    "value = az.Value()\n",
    "net = az.Network(policy, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save('network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing networks with replay buffer data\n",
    "\n",
    "See [this doc](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) for writing training loss to tensorboard data, and [this doc](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference) for saving/loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optim.Adam(policy.parameters())  #, lr=1e-5\n",
    "value_optimizer = optim.Adam(value.parameters())  #, lr=1e-5\n",
    "writer = SummaryWriter()\n",
    "global_step = 0  # how many minibatches the models have been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_step = az.train_step(rb, p, policy_optimizer, v, value_optimizer, writer, global_step=global_step, num_iters=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "Setting up this algorithm to run in parallel is quite important. I'm using [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) to handle the parallelism, and it looks like pytorch also has a similar API for moving Tensors around. With 2 processors on my laptop, speedup is about 90% (not bad...).\n",
    "\n",
    "Want to set random seed for each process, otherwise you end up getting all the same results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(x):\n",
    "    print(f'collecting data ({x})')\n",
    "    config = az.Config()\n",
    "    config.num_simulations = 250\n",
    "    ps_config = ps.PulseSequenceConfig(N=N, ensemble_size=ensemble_size,\n",
    "                                       max_sequence_length=max_sequence_length,\n",
    "                                       Utarget=Utarget,\n",
    "                                       pulse_width=pulse_width, delay=delay)\n",
    "    # load policy and value networks from memory\n",
    "    policy = az.Policy()\n",
    "    policy.load_state_dict(torch.load('network/policy'))\n",
    "    policy.eval()\n",
    "    value = az.Value()\n",
    "    value.load_state_dict(torch.load('network/value'))\n",
    "    value.eval()\n",
    "    net = az.Network(policy, value)\n",
    "    return az.make_sequence(config, ps_config, network=net, rng=ps_config.rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't select action: no child actions to perform!\n",
      "Can't select action: no child actions to perform!\n"
     ]
    }
   ],
   "source": [
    "# with mp.Pool(num_cores) as pool:\n",
    "#     output = pool.map(f, range(num_collect))\n",
    "# for stat in output:\n",
    "#     az.add_stats_to_buffer(stat, rb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together: training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f'on iteration {i}')\n",
    "    # collect data\n",
    "    print('collecting data...')\n",
    "    with mp.Pool(num_cores) as pool:\n",
    "        output = pool.map(collect_data, range(num_collect))\n",
    "    for stat in output:\n",
    "        az.add_stats_to_buffer(stat, rb)\n",
    "    mean_value = np.mean([o[-1][-1] for o in output])\n",
    "    for o in output:\n",
    "        if o[-1][-1] > 1:\n",
    "            print('Candidate pulse sequence found! Value is ', o[-1][-1], '\\n', o[-1][0])\n",
    "    writer.add_scalar('mean_value', mean_value, global_step=global_step)\n",
    "    # train models from replay buffer\n",
    "    print('training model...')\n",
    "    global_step = az.train_step(rb, policy, policy_optimizer,\n",
    "                                value, value_optimizer,\n",
    "                                writer, global_step=global_step,\n",
    "                                num_iters=num_iters, batch_size=batch_size)\n",
    "    # write updated weights to file\n",
    "    net.save('network')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
