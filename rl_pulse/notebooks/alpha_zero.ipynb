{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero implementation for pulse sequence design\n",
    "_Will Kaufman, December 2020_\n",
    "\n",
    "[Dalgaard et. al. (2020)](https://www.nature.com/articles/s41534-019-0241-0) applied this approach to constructing shaped pulses (as I understand it), but in theory this should be as applicable to pulse sequence design, if not more so. The original [AlphaZero paper](https://science.sciencemag.org/content/362/6419/1140.full) and the [AlphaGo Zero paper](https://www.nature.com/articles/nature24270) are useful resources.\n",
    "\n",
    "The general idea behind AlphaZero (as I understand it) is to do a \"smart\" tree search that balances previous knowledge (the policy), curiosity in unexplored branches, and high-value branches. My thought is that this can be improved with AHT (i.e. knowing that by the end of the pulse sequence, the pulse sequence must be cyclic (the overall frame transformation must be identity) and there must be equal times spent on each axis). This will provide a hard constraint that will (hopefully) speed up search.\n",
    "\n",
    "## System installation\n",
    "\n",
    "Make sure the following packages are installed\n",
    "\n",
    "- `numpy`\n",
    "- `scipy`\n",
    "- `qutip`\n",
    "- `pytorch`\n",
    "- `tensorboard`\n",
    "\n",
    "## TODO\n",
    "- [ ] Collect all hyperparameters up top or in config (e.g. how many pulse sequences to collect data from)\n",
    "- [ ] Speed up LSTM (save hidden state, batch parallel pulse sequences, other?)\n",
    "- [ ] Figure out GPU utilization (if I can...)\n",
    "- [ ] Look into collecting training data and training continuously\n",
    "- [ ] Change dirichlet noise to match number of possible moves (5 for now, eventually 24)\n",
    "- [ ] Dynamically figure out how many CPUs there are available, and set pool to use that\n",
    "- [ ] Mess around with hyperparameters (e.g. in config object), see if performance improves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [ ] Add other changes on github project page (lots of documenting algo run)\n",
    "- [ ] Run it on Discovery, hope it works!\n",
    "- [ ] Clean up code, add tests, make sure everything is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "from time import sleep\n",
    "import qutip as qt\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "import pulse_sequences as ps\n",
    "import alpha_zero as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(az)\n",
    "# importlib.reload(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_no_net_procs = 2  # 15\n",
    "collect_no_net_count = 5  # 100\n",
    "collect_procs = 2  # 15\n",
    "\n",
    "buffer_size = int(1e6)  # 1e6\n",
    "batch_size = 64  # 2048\n",
    "num_iters = int(1e2)  # 800e3\n",
    "\n",
    "max_sequence_length = 48\n",
    "\n",
    "print_every = 1  # 100\n",
    "save_every = 10  # 1000\n",
    "\n",
    "reward_threshold = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the spin system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dipolar_strength = 1e2\n",
    "pulse_width = 1e-5  # time is relative to chemical shift strength\n",
    "delay = 1e-4\n",
    "N = 3  # number of spins\n",
    "ensemble_size = 50\n",
    "rot_error = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, Z = ps.get_collective_spin(N)\n",
    "# Hsys_ensemble = [ps.get_Hsys(N) for _ in range(ensemble_size)]\n",
    "# pulses_ensemble = [\n",
    "#     ps.get_pulses(H, X, Y, Z, pulse_width, delay, rot_error=0.01) for H in Hsys_ensemble\n",
    "# ]\n",
    "# Utarget = qt.identity(Hsys_ensemble[0].dims[0])\n",
    "Utarget = qt.tensor([qt.identity(2)] * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smarter search with MCTS\n",
    "\n",
    "Following the [supplementary materials description under \"Search\"](https://science.sciencemag.org/content/sci/suppl/2018/12/05/362.6419.1140.DC1/aar6404-Silver-SM.pdf) to do rollouts and backpropagate information. All of the relevant code for the alpha zero algorithm is in alpha_zero.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of saving data in a reasonable way (and using RNN), the state is represented by a sequence, where 0 indicates the start of sequence, and 1-5 are the possible pulses (1: delay, 2: x, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = az.make_sequence(az.Config(), ps.PulseSequenceConfig(Utarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill replay buffer with inital data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data_no_net(proc_num, queue, ps_count, global_step, lock):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        proc_num: Which process number this is (for debug purposes)\n",
    "        queue (Queue): A queue to add the statistics gathered\n",
    "            from the MCTS rollouts.\n",
    "        ps_count (Value): Shared count of how many pulse sequences have\n",
    "            been constructed\n",
    "    \"\"\"\n",
    "    print(datetime.now(), f'collecting data without network ({proc_num})')\n",
    "    config = az.Config()\n",
    "    ps_config = ps.PulseSequenceConfig(N=N, ensemble_size=ensemble_size,\n",
    "                                       max_sequence_length=max_sequence_length,\n",
    "                                       Utarget=Utarget,\n",
    "                                       dipolar_strength=dipolar_strength,\n",
    "                                       pulse_width=pulse_width, delay=delay,\n",
    "                                       rot_error=rot_error)\n",
    "    for i in range(collect_no_net_count):\n",
    "        ps_config.reset()\n",
    "        output = az.make_sequence(config, ps_config, network=None,\n",
    "                                  rng=ps_config.rng)\n",
    "        if output[-1][2] > reward_threshold:\n",
    "            print(datetime.now(),\n",
    "                  f'candidate pulse sequence from {proc_num}',\n",
    "                  output[-1])\n",
    "        with lock:\n",
    "            queue.put(output)\n",
    "            ps_count.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     with mp.Manager() as manager:\n",
    "#         buffer = manager.list()  #[None] * buffer_size\n",
    "#         index = manager.Value(typecode='i', value=0)\n",
    "#         ps_count = manager.Value(typecode='i', value=0)\n",
    "#         lock = manager.RLock()\n",
    "#         workers = []\n",
    "#         for i in range(4):\n",
    "#             workers.append(mp.Process(target=collect_data_no_net,\n",
    "#                                       args=(i, buffer, index, lock,\n",
    "#                                             buffer_size, ps_count)))\n",
    "#             workers[-1].start()\n",
    "#         for w in workers:\n",
    "#             w.join()\n",
    "#         print('done gathering initial data!')\n",
    "#         l = list(buffer)  # to save a non-shared copy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing networks with replay buffer data\n",
    "\n",
    "See [this doc](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) for writing training loss to tensorboard data, and [this doc](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference) for saving/loading models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "Setting up this algorithm to run in parallel is quite important. I'm using [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) to handle the parallelism, and it looks like pytorch also has a similar API for moving Tensors around. With 2 processors on my laptop, speedup is about 90% (not bad...).\n",
    "\n",
    "Want to set random seed for each process, otherwise you end up getting all the same results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(proc_num, queue, net, ps_count, global_step, lock):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        queue (Queue): A queue to add the statistics gathered\n",
    "            from the MCTS rollouts.\n",
    "        ps_count (Value): A shared count of how many pulse sequences have been\n",
    "            constructed so far\n",
    "    \"\"\"\n",
    "    print(datetime.now(), f'collecting data ({proc_num})')\n",
    "    config = az.Config()\n",
    "    config.num_simulations = 250\n",
    "    ps_config = ps.PulseSequenceConfig(Utarget=Utarget, N=N,\n",
    "                                       ensemble_size=ensemble_size,\n",
    "                                       max_sequence_length=max_sequence_length,\n",
    "                                       dipolar_strength=dipolar_strength,\n",
    "                                       pulse_width=pulse_width, delay=delay,\n",
    "                                       rot_error=rot_error)\n",
    "    while global_step.value < num_iters:\n",
    "        ps_config.reset()\n",
    "        output = az.make_sequence(config, ps_config, network=net,\n",
    "                                  rng=ps_config.rng)\n",
    "        if output[-1][2] > reward_threshold:\n",
    "            print(datetime.now(),\n",
    "                  f'candidate pulse sequence from {proc_num}',\n",
    "                  output[-1])\n",
    "        with lock:\n",
    "            queue.put(output)\n",
    "            ps_count.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     with mp.Manager() as manager:\n",
    "#         buffer = manager.list()  #[None] * 500\n",
    "#         index = manager.Value(typecode='i', value=0)\n",
    "#         lock = manager.RLock()\n",
    "#         # get network\n",
    "#         net = az.Network()\n",
    "#         net.share_memory()\n",
    "#         workers = []\n",
    "#         for i in range(4):\n",
    "#             workers.append(mp.Process(target=collect_data,\n",
    "#                                       args=(i, buffer, index, lock, buffer_size, net)))\n",
    "#             workers[-1].start()\n",
    "#         for w in workers:\n",
    "#             w.join()\n",
    "#         print('done gathering data!')\n",
    "#         l = list(buffer)  # save a non-shared copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(queue, net, global_step, ps_count, lock,\n",
    "                  c_value=1e0, c_l2=1e-6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        queue (Queue): A queue to add the statistics gathered\n",
    "            from the MCTS rollouts.\n",
    "        global_step (mp.managers.Value): Counter to keep track\n",
    "            of training iterations\n",
    "        writer (SummaryWriter): Write losses to log\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter()\n",
    "    start_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    net_optimizer = optim.Adam(net.parameters(),)\n",
    "    # construct replay buffer locally\n",
    "    buffer = []\n",
    "    index = 0\n",
    "    i = 0\n",
    "    # write network structure to tensorboard file\n",
    "    tmp = torch.zeros((1, 10, 6))\n",
    "    writer.add_graph(net, tmp)\n",
    "    del tmp\n",
    "    while global_step.value < num_iters:  # number of training iterations\n",
    "        if i % save_every == 0:\n",
    "            print(datetime.now(), 'saving network...')\n",
    "            # save network parameters to file\n",
    "            if not os.path.exists(f'{start_time}-network'):\n",
    "                os.makedirs(f'{start_time}-network')\n",
    "            torch.save(net.state_dict(), f'{start_time}-network/{i:07.0f}-network')\n",
    "        # check if queue has new data to add to replay buffer\n",
    "        with lock:\n",
    "            while not queue.empty():\n",
    "                new_stats = queue.get()\n",
    "                new_stats = az.convert_stats_to_tensors(new_stats)\n",
    "                for stat in new_stats:\n",
    "                    if len(buffer) < buffer_size:\n",
    "                        buffer.append(stat)\n",
    "                    else:\n",
    "                        buffer[index] = stat\n",
    "                    index = index + 1 if index < buffer_size - 1 else 0\n",
    "        # carry on with training\n",
    "        if len(buffer) < batch_size:\n",
    "            print(datetime.now(), 'not enough data yet, sleeping...')\n",
    "            sleep(5)\n",
    "            continue\n",
    "#         elif len(buffer) < 1e4:\n",
    "#             sleep(.5)  # put on the brakes a bit, don't tear through the data\n",
    "        net_optimizer.zero_grad()\n",
    "        # sample minibatch from replay buffer\n",
    "        minibatch = random.sample(buffer, batch_size)\n",
    "        states, probabilities, values = zip(*minibatch)\n",
    "        probabilities = torch.stack(probabilities)\n",
    "        values = torch.stack(values)\n",
    "        packed_states = az.pad_and_pack(states)\n",
    "        # evaluate network\n",
    "        policy_outputs, value_outputs, _ = net(packed_states)\n",
    "        policy_loss = -1 / \\\n",
    "            len(states) * torch.sum(probabilities * torch.log(policy_outputs))\n",
    "        value_loss = F.mse_loss(value_outputs, values)\n",
    "        l2_reg = torch.tensor(0.)\n",
    "        for param in net.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "        loss = policy_loss + c_value * value_loss + c_l2 * l2_reg\n",
    "        loss.backward()\n",
    "        net_optimizer.step()\n",
    "        # write losses to log\n",
    "        writer.add_scalar('training_policy_loss',\n",
    "                          policy_loss, global_step=global_step.value)\n",
    "        writer.add_scalar('training_value_loss',\n",
    "                          c_value * value_loss, global_step=global_step.value)\n",
    "        writer.add_scalar('training_l2_reg',\n",
    "                          c_l2 * l2_reg, global_step=global_step.value)\n",
    "        \n",
    "        # every 10 iterations, add histogram of replay buffer values\n",
    "        # and save network to file...\n",
    "        if i % print_every == 0:\n",
    "            print(datetime.now(), f'updated network (iteration {i})',\n",
    "                  f'pulse_sequence_count: {ps_count.value}')\n",
    "            _, _, values = zip(*list(buffer))\n",
    "            values = torch.stack(values).squeeze()\n",
    "            writer.add_histogram('buffer_values', values,\n",
    "                                 global_step=global_step.value)\n",
    "            writer.add_scalar('pulse_sequence_count', ps_count.value,\n",
    "                              global_step.value)\n",
    "        with lock:\n",
    "            global_step.value += 1\n",
    "        i += 1\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    with mp.Manager() as manager:\n",
    "        queue = manager.Queue()\n",
    "        global_step = manager.Value('i', 0)\n",
    "        ps_count = manager.Value('i', 0)\n",
    "        lock = manager.Lock()\n",
    "        # get network\n",
    "        net = az.Network()\n",
    "        # optionally load state dict\n",
    "        # net.load_state_dict(torch.load('network_state'))\n",
    "        net.share_memory()\n",
    "        collectors = []\n",
    "        for i in range(collect_no_net_procs):\n",
    "            c = mp.Process(target=collect_data_no_net,\n",
    "                           args=(i, queue, ps_count, global_step, lock))\n",
    "            c.start()\n",
    "            collectors.append(c)\n",
    "        trainer = mp.Process(target=train_process,\n",
    "                             args=(queue, net,\n",
    "                                   global_step, ps_count, lock))\n",
    "        trainer.start()\n",
    "        # join collectors before starting more\n",
    "        for c in collectors:\n",
    "            c.join()\n",
    "        collectors.clear()\n",
    "        # start data collectors with network\n",
    "        for i in range(collect_procs):\n",
    "            c = mp.Process(target=collect_data,\n",
    "                           args=(i, queue, net, ps_count, global_step, lock))\n",
    "            c.start()\n",
    "            collectors.append(c)\n",
    "        for c in collectors:\n",
    "            c.join()\n",
    "        print('all collectors are joined')\n",
    "        trainer.join()\n",
    "        print('trainer is joined')\n",
    "        print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that sharing the neural network behaves as expected! Training updates the weights, and those updated weights are reflected in each of the data collection processes. Neat!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
