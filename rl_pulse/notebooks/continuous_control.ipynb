{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse design using reinforcement learning\n",
    "_Written by Will Kaufman, November 2020_\n",
    "\n",
    "This notebook walks through a reinforcement learning approach to pulse design for spin systems.\n",
    "\n",
    "**TODO** fill in this introduction more!\n",
    "\n",
    "To view tensorboard output, run `tensorboard --logdir logs/` in the current directory.\n",
    "\n",
    "## Running this code on Discovery\n",
    "\n",
    "This notebook (with a little setup) could be run on Discovery. Alternatively, this notebook can be saved as a `.py` file and that Python file can then be executed. Make sure to do the following though:\n",
    "\n",
    "- [ ] Delete all comments for shorter code/suppress errors (use regex: `( *)#.*?$` and replace)\n",
    "- [ ] Make sure number of iterations at end of file is set properly (currently set to 5, should be around 1000)\n",
    "- [ ] Add hyperparamter settings by pulling `sys.argv` values (e.g. for $\\epsilon$, $c_1$, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import qutip as qt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "sys.path.append('../..')  # for running jobs on Discovery\n",
    "\n",
    "from rl_pulse.environments import spin_system_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(spin_system_continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define algorithm hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO eventually fill in hyperparameters at top of doc\n",
    "discount = 0.99\n",
    "stddev = 1e-3\n",
    "epsilon = .2\n",
    "c1 = 1e2\n",
    "num_epochs = 10\n",
    "minibatch_size = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the spin system\n",
    "\n",
    "This sets the parameters of the system ($N$ spin-1/2 particles, which corresponds to a Hilbert space with dimension $2^N$). For the purposes of simulation, $\\hbar \\equiv 1$.\n",
    "\n",
    "The total internal Hamiltonian is given by\n",
    "$$\n",
    "H_\\text{int} = C H_\\text{dip} + \\sum_i^N \\delta_i I_z^{i}\n",
    "$$\n",
    "where $C$ is the coupling strength, $\\delta$ is the chemical shift strength (each spin is assumed to be identical), and $H_\\text{dip}$ is given by\n",
    "$$\n",
    "H_\\text{dip} = \\sum_{i,j}^N d_{i,j} \\left(3I_z^{i}I_z^{j} - \\mathbf{I}^{i} \\cdot \\mathbf{I}^{j}\\right)\n",
    "$$\n",
    "\n",
    "The target unitary transformation is a simple $\\pi/2$-pulse about the x-axis\n",
    "$$\n",
    "U_\\text{target} = \\exp\\left(-i \\frac{\\pi}{4} \\sum_j I_x^j \\right)\n",
    "$$\n",
    "\n",
    "<!-- Hamiltonian is set to be the 0th-order average Hamiltonian from the WHH-4 pulse sequence, which is designed to remove the dipolar interaction term from the internal Hamiltonian. The pulse sequence is $\\tau, \\overline{X}, \\tau, Y, \\tau, \\tau, \\overline{Y}, \\tau, X, \\tau$.\n",
    "The zeroth-order average Hamiltonian for the WAHUHA pulse sequence is\n",
    "$$\n",
    "H_\\text{WHH}^{(0)} = \\delta / 3 \\sum_i^N \\left( I_x^{i} + I_y^{i} + I_z^{i} \\right)\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3  # 4-spin system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemical_shifts = np.random.normal(scale=50, size=(N,))\n",
    "Hcs = sum(\n",
    "    [qt.tensor(\n",
    "        [qt.identity(2)]*i\n",
    "        + [chemical_shifts[i] * qt.sigmaz()]\n",
    "        + [qt.identity(2)]*(N-i-1)\n",
    "    ) for i in range(N)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dipolar_matrix = np.random.normal(scale=50, size=(N, N))\n",
    "Hdip = sum([\n",
    "    dipolar_matrix[i, j] * (\n",
    "        2 * qt.tensor(\n",
    "            [qt.identity(2)]*i\n",
    "            + [qt.sigmaz()]\n",
    "            + [qt.identity(2)]*(j-i-1)\n",
    "            + [qt.sigmaz()]\n",
    "            + [qt.identity(2)]*(N-j-1)\n",
    "        )\n",
    "        - qt.tensor(\n",
    "            [qt.identity(2)]*i\n",
    "            + [qt.sigmax()]\n",
    "            + [qt.identity(2)]*(j-i-1)\n",
    "            + [qt.sigmax()]\n",
    "            + [qt.identity(2)]*(N-j-1)\n",
    "        )\n",
    "        - qt.tensor(\n",
    "            [qt.identity(2)]*i\n",
    "            + [qt.sigmay()]\n",
    "            + [qt.identity(2)]*(j-i-1)\n",
    "            + [qt.sigmay()]\n",
    "            + [qt.identity(2)]*(N-j-1)\n",
    "        )\n",
    "    )\n",
    "    for i in range(N) for j in range(i+1, N)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hsys = Hcs + Hdip\n",
    "X = sum(\n",
    "    [qt.tensor(\n",
    "        [qt.identity(2)]*i\n",
    "        + [qt.sigmax()]\n",
    "        + [qt.identity(2)]*(N-i-1)\n",
    "    ) for i in range(N)]\n",
    ")\n",
    "Y = sum(\n",
    "    [qt.tensor(\n",
    "        [qt.identity(2)]*i\n",
    "        + [qt.sigmay()]\n",
    "        + [qt.identity(2)]*(N-i-1)\n",
    "    ) for i in range(N)]\n",
    ")\n",
    "Hcontrols = [50e3 * X, 50e3 * Y]\n",
    "target = qt.propagator(X, np.pi/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SpinSystemContinuousEnv` simulates the quantum system given above, and exposes relevant methods for RL (including a `step` method that takes an action and returns an observation and reward, a `reset` method to reset the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = spin_system_continuous.SpinSystemContinuousEnv(\n",
    "    Hsys=Hsys,\n",
    "    Hcontrols=Hcontrols,\n",
    "    target=target,\n",
    "    discount=discount,\n",
    "    infidelity_threshold=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pg_metric = tf.keras.metrics.Mean('loss_pg', dtype=tf.float32)\n",
    "loss_vf_metric = tf.keras.metrics.Mean('loss_vf', dtype=tf.float32)\n",
    "infidelity_metric = tf.keras.metrics.Mean('infidelity', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = os.path.join('logs', current_time, 'train')\n",
    "test_log_dir = os.path.join('logs', current_time, 'test')\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "if not os.path.exists(os.path.join(\n",
    "    'logs', current_time, 'controls'\n",
    ")):\n",
    "    os.makedirs(os.path.join(\n",
    "        'logs', current_time, 'controls'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of time steps played through\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define actor and critic networks\n",
    "\n",
    "The observations of the system are sequences of control amplitudes that have been performed on the system (which most closely represents the knowledge of a typical experimental system).\n",
    "\n",
    "The actor and critic networks can be completely separate (using different neural networks and trained separately) or can share some layers (e.g. sharing an LSTM layer to convert the sequence of control amplitudes to a hidden state, and two dense layers). In that case, separate policy and value \"heads\" are used for the two different networks. This approach adds some regularization, but risks disrupting the policy from updates to the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.layers.LSTM(64)\n",
    "stateful_lstm = tf.keras.layers.LSTM(64, stateful=True)\n",
    "hidden1 = tf.keras.layers.Dense(64, activation=tf.keras.activations.relu)\n",
    "# hidden2 = tf.keras.layers.Dense(64, activation=tf.keras.activations.relu)\n",
    "policy = tf.keras.layers.Dense(2, activation=tf.keras.activations.tanh)\n",
    "value = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = tf.keras.models.Sequential([\n",
    "    lstm,\n",
    "    hidden1,\n",
    "#     hidden2,\n",
    "    policy\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_critic = tf.keras.layers.LSTM(64)\n",
    "hidden2 = tf.keras.layers.Dense(64, activation=tf.keras.activations.relu)\n",
    "value = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net = tf.keras.models.Sequential([\n",
    "#     lstm,\n",
    "    lstm_critic,\n",
    "#     hidden1,\n",
    "    hidden2,\n",
    "    value\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_actor_net = tf.keras.models.Sequential([\n",
    "    stateful_lstm,\n",
    "    hidden1,\n",
    "#     hidden2,\n",
    "    policy\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define PPO agent\n",
    "\n",
    "[Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) is a state-of-the-art RL algorithm that can be used for both discrete and continuous action spaces. PPO prevents the policy from over-adjusting during training by defining a clipped policy gradient loss function:\n",
    "$$\n",
    "L^\\text{clip}(\\theta) = \\mathbb{E}_t\\left[\n",
    "\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(\n",
    "    r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\n",
    ")\\hat{A}_t\n",
    "\\right]\n",
    "$$\n",
    "where the \"importance ratio\" $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_\\text{old}}(a_t|s_t)}$ is the relative probability of choosing the action under the new policy compared to the old policy. By clipping the loss function, there is non-zero gradient only in a small region around the original policy.\n",
    "\n",
    "Because the actor and critic networks share layers, the total loss function is used for training\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_t \\left[\n",
    "-L^\\text{clip}(\\theta) + c_1 L^\\text{VF}(\\theta)\n",
    "\\right]\n",
    "$$\n",
    "with $L^\\text{VF}(\\theta)$ as the MSE loss for value estimates.\n",
    "\n",
    "Basing off TF-Agents [abstract base class](https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/TFAgent). Also using [PPOAgent code](https://github.com/tensorflow/agents/blob/v0.6.0/tf_agents/agents/ppo/ppo_agent.py#L746)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect some experience from the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following collects experience by interacting with the environment.\n",
    "\n",
    "All the data should have dimensions `batch_size * [other dims]`, shouldn't just be `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, step_types, discounts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        rewards: A tensor of rewards for the episode.\n",
    "            Should have size batch_size * 1.\n",
    "        step_types: A tensor of step types, 1 if a normal step.\n",
    "            Should have size batch_size * 1.\n",
    "    \"\"\"\n",
    "    returns = [0] * rewards.shape[0]\n",
    "    returns[-1] = rewards[-1]\n",
    "    for i in range(1, len(rewards)):\n",
    "        returns[-(i + 1)] = (discounts[-i]\n",
    "                             * returns[-i]\n",
    "                             * tf.cast(step_types[-(i + 1)] == 1,\n",
    "                                       tf.float32)) + rewards[-(i+1)]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_and_mask(obs_list, max_sequence_length=100):\n",
    "    obs = []\n",
    "    mask = []\n",
    "    num_features = obs_list[0].shape[-1]\n",
    "    for i in range(len(obs_list)):\n",
    "        obs_length = obs_list[i].shape[-2]\n",
    "        obs.append(tf.concat(\n",
    "            [tf.cast(obs_list[i], tf.float32),\n",
    "             tf.zeros((1,\n",
    "                       max_sequence_length - obs_length,\n",
    "                       num_features))],\n",
    "            axis=1\n",
    "        ))\n",
    "        mask.append(tf.concat(\n",
    "            [tf.ones((1, obs_length)),\n",
    "             tf.zeros((1,\n",
    "                       max_sequence_length - obs_length))],\n",
    "            axis=1\n",
    "        ))\n",
    "    obs = tf.squeeze(tf.stack(obs))\n",
    "    mask = tf.squeeze(tf.stack(mask))\n",
    "    return obs, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(\n",
    "        num_steps=100,\n",
    "        stddev=1e-3,\n",
    "        max_sequence_length=100):\n",
    "    \"\"\"Collect experience from the environment.\n",
    "    Starts with a new episode and collects num_steps\n",
    "    \n",
    "    Args:\n",
    "        num_steps: How many steps to collect from the environment.\n",
    "        stddev: Standard deviation of noise applied to actions.\n",
    "        max_sequence_length: For sequence data, how long to make\n",
    "            the max sequence.\n",
    "    \"\"\"\n",
    "    step_types = []\n",
    "    discounts = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    action_means = []\n",
    "    rewards = []\n",
    "    step = env.reset()\n",
    "    if stateful_actor_net.built:\n",
    "        stateful_actor_net.reset_states()\n",
    "    # collect experience\n",
    "    for _ in range(num_steps):\n",
    "        observations.append(step.observation)\n",
    "        action_mean = stateful_actor_net(\n",
    "            tf.expand_dims(step.observation[:, -1, :], 1)\n",
    "        )\n",
    "        action_means.append(action_mean)\n",
    "        action = action_mean + tf.random.normal(shape=action_mean.shape,\n",
    "                                                stddev=stddev)\n",
    "        actions.append(action)\n",
    "        step = env.step(action)\n",
    "        rewards.append(step.reward)\n",
    "        step_types.append(step.step_type)\n",
    "        discounts.append(step.discount)\n",
    "        if step.step_type == 2:\n",
    "            # episode is done, reset environment and network state\n",
    "            stateful_actor_net.reset_states()\n",
    "            infidelity_metric(1 - env.fidelity())\n",
    "            step = env.reset()\n",
    "    # put data into tensors\n",
    "    step_types = tf.stack(step_types)\n",
    "    discounts = tf.stack(discounts)\n",
    "    actions = tf.squeeze(tf.stack(actions))\n",
    "    action_means = tf.squeeze(tf.stack(action_means))\n",
    "    rewards = tf.stack(rewards)\n",
    "    # reshape observations to be same sequence length, and create\n",
    "    # a mask for original sequence length\n",
    "    obs, mask = get_obs_and_mask(observations,\n",
    "                                 max_sequence_length=max_sequence_length)\n",
    "    returns = tf.stack(calculate_returns(rewards, step_types, discounts))\n",
    "    advantages = returns - critic_net(obs, mask=mask)\n",
    "    old_action_log_probs = tf.reduce_sum(\n",
    "        -((actions - action_means) / stddev)**2,\n",
    "        axis=1,\n",
    "        keepdims=True\n",
    "    )\n",
    "    return (\n",
    "        obs, mask, actions, action_means,\n",
    "        rewards, step_types, discounts, returns,\n",
    "        advantages, old_action_log_probs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f collect_experience collect_experience()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     obs, mask, actions, action_means,\n",
    "#     rewards, step_types, discounts, returns,\n",
    "#     advantages, old_action_log_probs\n",
    "# ) = collect_experience(num_steps=500,\n",
    "#                        stddev=stddev,\n",
    "#                        max_sequence_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_actor():\n",
    "    \"\"\"Evaluate the actor.\n",
    "    \n",
    "    Returns: A tuple containing\n",
    "        rewards: A tensor of rewards for the episode.\n",
    "        control_amplitudes: A tensor of control amplitudes\n",
    "            applied during the episode.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    step = env.reset()\n",
    "    if stateful_actor_net.built:\n",
    "        stateful_actor_net.reset_states()\n",
    "    while step.step_type <= 1:\n",
    "        action = stateful_actor_net(\n",
    "            tf.expand_dims(step.observation[:, -1, :], 1)\n",
    "        )\n",
    "        step = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(step.reward)\n",
    "    rewards = tf.stack(rewards)\n",
    "    infidelity_metric(1 - env.fidelity())\n",
    "    return rewards, tf.squeeze(step.observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = evaluate_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(b.numpy()[:,0], label='x')\n",
    "# plt.plot(b.numpy()[:,1], label='y')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To \"train\" the actor and critic networks (change the network parameters to minimize the loss function), an optimizer using the Adam algorithm is used. The loss function is described above, and is composed of policy-gradient loss and value function loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "mse = tf.losses.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not critic_net.built:\n",
    "    critic_net.build(input_shape=(None, None, 2))\n",
    "if not actor_net.built:\n",
    "    actor_net.build(input_shape=(None, None, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of trainable variables that should be updated when minimizing the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_vars = critic_net.trainable_variables\n",
    "actor_vars = actor_net.trainable_variables\n",
    "trainable_variables = set()\n",
    "for var in critic_vars + actor_vars:\n",
    "    trainable_variables.add(var.ref())\n",
    "trainable_variables = list(trainable_variables)\n",
    "trainable_variables = [var.deref() for var in trainable_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(\n",
    "        actor_net,\n",
    "        critic_net,\n",
    "        trainable_variables,\n",
    "        obs,\n",
    "        mask,\n",
    "        actions,\n",
    "        action_means,\n",
    "        old_action_log_probs,\n",
    "        returns,\n",
    "        advantages,\n",
    "        stddev=1e-3,\n",
    "        epsilon=.2,\n",
    "        c1=1):\n",
    "    \"\"\"\n",
    "    Returns: tuple containing\n",
    "        l: Total loss.\n",
    "        grad: Gradient of loss wrt trainable variables.\n",
    "    \"\"\"\n",
    "    batch_size = obs.shape[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_log_probs = (\n",
    "            tf.reduce_sum(-((actions - actor_net(obs, mask)) / stddev)**2,\n",
    "                          axis=1,\n",
    "                          keepdims=True))\n",
    "        importance_ratio = tf.exp(action_log_probs - old_action_log_probs)\n",
    "        loss_pg = tf.reduce_sum(tf.minimum(\n",
    "            importance_ratio * advantages,\n",
    "            tf.clip_by_value(\n",
    "                importance_ratio,\n",
    "                1 - epsilon,\n",
    "                1 + epsilon) * advantages\n",
    "        )) / batch_size\n",
    "        loss_vf = mse(tf.squeeze(returns), tf.squeeze(critic_net(obs, mask)))\n",
    "        total_loss = -loss_pg + c1 * loss_vf\n",
    "    grads = tape.gradient(total_loss, trainable_variables)\n",
    "    # record loss values to metrics\n",
    "    loss_pg_metric(loss_pg)\n",
    "    loss_vf_metric(loss_vf)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch(\n",
    "        obs,\n",
    "        mask,\n",
    "        actions,\n",
    "        action_means,\n",
    "        old_action_log_probs,\n",
    "        returns,\n",
    "        advantages,\n",
    "        stddev=1e-3,\n",
    "        epsilon=.2,\n",
    "        c1=1,\n",
    "        num_epochs=10,\n",
    "        minibatch_size=50):\n",
    "    for i in range(num_epochs):\n",
    "        minibatch = np.random.choice(obs.shape[0], size=minibatch_size)\n",
    "        grads = calculate_gradients(\n",
    "            actor_net, critic_net, trainable_variables,\n",
    "            tf.gather(obs, indices=minibatch),\n",
    "            tf.gather(mask, indices=minibatch),\n",
    "            tf.gather(actions, indices=minibatch),\n",
    "            tf.gather(action_means, indices=minibatch),\n",
    "            tf.gather(old_action_log_probs, indices=minibatch),\n",
    "            tf.gather(returns, indices=minibatch),\n",
    "            tf.gather(advantages, indices=minibatch),\n",
    "            stddev=stddev,\n",
    "            epsilon=epsilon,\n",
    "            c1=c1)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train minibatch, record the loss values and infidelity for the episode, and update layers with new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # increment global step by number of timesteps that are being trained on\n",
    "# global_step.assign_add(obs.shape[0])\n",
    "# train_minibatch(\n",
    "#     obs,\n",
    "#     mask,\n",
    "#     actions,\n",
    "#     action_means,\n",
    "#     old_action_log_probs,\n",
    "#     returns,\n",
    "#     advantages,\n",
    "#     stddev=1e-3,\n",
    "#     epsilon=.2,\n",
    "#     c1=1,\n",
    "#     num_epochs=10,\n",
    "#     minibatch_size=50\n",
    "# )\n",
    "\n",
    "# with train_summary_writer.as_default():\n",
    "#     global_step_np = global_step.numpy()\n",
    "#     tf.summary.scalar('loss_pg', loss_pg_metric.result(), step=global_step_np)\n",
    "#     tf.summary.scalar('loss_vf', loss_vf_metric.result(), step=global_step_np)\n",
    "#     tf.summary.scalar('infidelity', infidelity_metric.result(), step=global_step_np)\n",
    "\n",
    "# loss_pg_metric.reset_states()\n",
    "# loss_vf_metric.reset_states()\n",
    "# infidelity_metric.reset_states()\n",
    "# stateful_actor_net.layers[0].set_weights(actor_net.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a PPO experience collection and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loop(\n",
    "        stddev=1e-2,\n",
    "        epsilon=.2,\n",
    "        c1=1,\n",
    "        num_epochs=5,\n",
    "        minibatch_size=25,\n",
    "        save_weights=False,\n",
    "        evaluate=False):\n",
    "    \"\"\"Collects experience, trains networks\n",
    "    \"\"\"\n",
    "    (\n",
    "        obs, mask, actions, action_means,\n",
    "        rewards, step_types, discounts, returns,\n",
    "        advantages, old_action_log_probs\n",
    "    ) = collect_experience(\n",
    "        num_steps=500,\n",
    "        stddev=stddev,\n",
    "        max_sequence_length=100\n",
    "    )\n",
    "    print('collected experience')\n",
    "    global_step.assign_add(obs.shape[0])\n",
    "    global_step_np = global_step.numpy()\n",
    "    # train\n",
    "    train_minibatch(\n",
    "        obs,\n",
    "        mask,\n",
    "        actions,\n",
    "        action_means,\n",
    "        old_action_log_probs,\n",
    "        returns,\n",
    "        advantages,\n",
    "        stddev=stddev,\n",
    "        epsilon=epsilon,\n",
    "        c1=c1,\n",
    "        num_epochs=num_epochs,\n",
    "        minibatch_size=minibatch_size)\n",
    "    print('trained networks')\n",
    "    stateful_actor_net.layers[0].set_weights(actor_net.layers[0].get_weights())\n",
    "    stateful_actor_net.reset_states()\n",
    "    print('reset state')\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss_pg', \n",
    "                          loss_pg_metric.result(), \n",
    "                          step=global_step_np)\n",
    "        tf.summary.scalar('loss_vf', \n",
    "                          loss_vf_metric.result(), \n",
    "                          step=global_step_np)\n",
    "        tf.summary.scalar('infidelity', \n",
    "                          infidelity_metric.result(), \n",
    "                          step=global_step_np)\n",
    "    loss_pg_metric.reset_states()\n",
    "    loss_vf_metric.reset_states()\n",
    "    infidelity_metric.reset_states()\n",
    "    if evaluate:\n",
    "        # evaluate the actor with noise-free actions\n",
    "        rewards, control_amplitudes = evaluate_actor()\n",
    "        np.savez_compressed(\n",
    "            os.path.join(\n",
    "                'logs', current_time,\n",
    "                'controls', f'{global_step.numpy():06.0f}'),\n",
    "            control_amplitudes=control_amplitudes.numpy()\n",
    "        )\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('infidelity', \n",
    "                              infidelity_metric.result(),\n",
    "                              step=global_step_np)\n",
    "        infidelity_metric.reset_states()\n",
    "    print('recorded metrics')\n",
    "    if save_weights:\n",
    "        actor_net.save_weights(os.path.join(\n",
    "            'logs', current_time,\n",
    "            'models', f'actor-{global_step.numpy():06.0f}'\n",
    "        ))\n",
    "        critic_net.save_weights(os.path.join(\n",
    "            'logs', current_time,\n",
    "            'models', f'critic-{global_step.numpy():06.0f}'\n",
    "        ))\n",
    "        print('saved model weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(int(5)):\n",
    "    print(f'iteration {i}')\n",
    "    save_weights = i % 25 == 0\n",
    "    evaluate = i % 10 == 0\n",
    "    ppo_loop(\n",
    "        stddev=stddev,\n",
    "        epsilon=epsilon,\n",
    "        c1=c1,\n",
    "        num_epochs=num_epochs,\n",
    "        minibatch_size=minibatch_size,\n",
    "        save_weights=save_weights,\n",
    "        evaluate=evaluate\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
