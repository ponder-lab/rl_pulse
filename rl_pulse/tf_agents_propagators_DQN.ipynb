{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design using DQN\n",
    "_Written by Will Kaufman_\n",
    "\n",
    "This notebook walks through a reinforcement learning approach to pulse sequence design for spin systems. [TF-Agents](https://www.tensorflow.org/agents) is used as a reinforcement learning library that uses Tensorflow, a common machine learning framework.\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Figure out if an RNN starts with empty initial state (even if start of trajectory isn't initial state), or if it starts with initial state saved in replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import spin_simulation as ss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network, q_rnn_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from environments import spin_sys_discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'environments.spin_sys_discrete' from '/Users/willkaufman/projects/rl_pulse/rl_pulse/environments/spin_sys_discrete.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(spin_sys_discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define algorithm hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "episode_length = 5 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = episode_length  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 12 #64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 5  # @param {type:\"integer\"}\n",
    "eval_interval = 200  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the spin system\n",
    "\n",
    "This sets the parameters of the system ($N$ spin-1/2 particles, which corresponds to a Hilbert space with dimension $2^N$). For the purposes of simulation, $\\hbar \\equiv 1$.\n",
    "\n",
    "The total internal Hamiltonian is given by\n",
    "$$\n",
    "H_\\text{int} = C H_\\text{dip} + \\delta \\sum_i^N I_z^{i}\n",
    "$$\n",
    "where $C$ is the coupling strength, $\\delta$ is the chemical shift strength (each spin is assumed to be identical), and $H_\\text{dip}$ is given by\n",
    "$$\n",
    "H_\\text{dip} = \\sum_{i,j}^N d_{i,j} \\left(3I_z^{i}I_z^{j} - \\mathbf{I}^{i} \\cdot \\mathbf{I}^{j}\\right)\n",
    "$$\n",
    "\n",
    "The target Hamiltonian is set to be the 0th-order average Hamiltonian from the WHH-4 pulse sequence, which is designed to remove the dipolar interaction term from the internal Hamiltonian. The pulse sequence is $\\tau, \\overline{X}, \\tau, Y, \\tau, \\tau, \\overline{Y}, \\tau, X, \\tau$.\n",
    "The zeroth-order average Hamiltonian for the WAHUHA pulse sequence is\n",
    "$$\n",
    "H_\\text{WHH}^{(0)} = \\delta / 3 \\sum_i^N \\left( I_x^{i} + I_y^{i} + I_z^{i} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=4\n",
    "dim = 2**N\n",
    "coupling = 1e3\n",
    "delta = 500\n",
    "(X,Y,Z) = ss.get_total_spin(N=N, dim=dim)\n",
    "H_target = ss.get_H_WHH_0(X, Y, Z, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SpinSystemDiscreteEnv` class keeps track of the system dynamics, and implements methods that are necessary for RL:\n",
    "\n",
    "- `action_spec`: Returns an `ArraySpec` that gives the shape and range of a valid action. For example, in a discrete action space, an action will be an integer scalar between 0 and `numActions - 1`. For a continuous action space, an action will be a 3-dimensional vector representing phase, amplitude, and duration of the pulse.\n",
    "- `observation_spec`: Returns an `ArraySpec` that gives the shape and range of a valid observation. In this case, the observations are all the actions performed on the environment so far.\n",
    "- `_reset`: Resets the environment. This means setting the propagator to the identity, and choosing a new random dipolar interaction matrix $(d_{i,j})$.\n",
    "- `_step`: Evolves the environment according to the action. Returns a `TimeStep` which includes the step type (`FIRST`, `MID`, or `LAST`), the **reward**, the discount rate to apply to future rewards, and an **observation** of the environment.\n",
    "\n",
    "The reward function $r(s,a)$ can in general depend on the environment state _and_ action performed. However, because the goal of pulse sequence design is to find high-fidelity pulse sequences, the reward only depends on the state. \n",
    "$$\n",
    "r = -\\log \\left( 1-\n",
    "    \\left|\n",
    "        \\frac{\\text{Tr} (U_\\text{target}^\\dagger U_\\text{exp})}{\\text{Tr}(\\mathbb{1})}\n",
    "    \\right|\n",
    "    \\right)\n",
    "% = -\\log\\left( 1- \\text{fidelity}(U_\\text{target}, U_\\text{exp}) \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "ArraySpec(shape=(16, 16, 4), dtype=dtype('float32'), name=None)\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int32'), name=None, minimum=0, maximum=4)\n"
     ]
    }
   ],
   "source": [
    "env = spin_sys_discrete.SpinSystemDiscreteEnv(N=4, dim=16, coupling=1e3,\n",
    "    delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "    delay_after=True, state_size=episode_length)\n",
    "# env.reset()\n",
    "\n",
    "# train_py_env = spin_sys_discrete.SpinSystemDiscreteEnv(N=4, dim=16, coupling=1e3,\n",
    "#     delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "#     delay_after=True)\n",
    "# eval_py_env = spin_sys_discrete.SpinSystemDiscreteEnv(N=4, dim=16, coupling=1e3,\n",
    "#     delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "#     delay_after=True)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "\n",
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)\n",
    "\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Q-network\n",
    "\n",
    "For the DQN algorithm, a Q-network must be defined. The Q-function $Q^\\pi(s,a)$ approximates the total return from performing action $a$ in state $s$, then following policy $\\pi$.\n",
    "<!-- $$\n",
    "Q^\\pi(s,a) = \n",
    "$$ -->\n",
    "\n",
    "Because the system's state is entirely determined by the sequence of actions performed, a RNN is used for the Q-network. This means that the network has an internal state that acts as a memory of the episode. Each observation passed to the Q-network updates the internal state, and the internal state is reset at the end of every episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    conv_layer_params=[(32, 5, 1), (32, 3, 1), (16, 3, 1)]\n",
    ")\n",
    "\n",
    "target_q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    conv_layer_params=[(32, 5, 1), (32, 3, 1), (16, 3, 1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the initial Q-values are for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20589064, -0.19506574, -0.18172377, -0.19854292, -0.22036171]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_net(train_env.current_time_step().observation)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"QNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  97019     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  205       \n",
      "=================================================================\n",
      "Total params: 97,224\n",
      "Trainable params: 97,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EncodingNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  3232      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  4624      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  76875     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  3040      \n",
      "=================================================================\n",
      "Total params: 97,019\n",
      "Trainable params: 97,019\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_net.get_layer(\"EncodingNetwork\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_net.get_layer(\"EncodingNetwork\").get_layer(\"dense_42\").get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent\n",
    "\n",
    "In RL, the \"agent\" has a policy that determines its behavior. For DQN, the agent will act greedily during evaluation (i.e. it picks the action with the maximal Q-value) and epsilon-greedily during data collection. These policies are accessed with `agent.policy` (for evaluation) and `agent.collect_policy` (for data collection).\n",
    "\n",
    "According to [the docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/tf_agent/TFAgent?hl=fa#args), I can adjust `train_sequence_length=None` for RNN-based agents. When using non-RNN DQN, though, I don't have that option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    target_q_network=target_q_net,\n",
    "    target_update_period=10,\n",
    "    optimizer=optimizer,\n",
    "    gamma=0.99,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10, print_actions=False):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        policy_state = policy.get_initial_state(environment.batch_size)\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step, policy_state = policy_state)\n",
    "            policy_state = action_step.state\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            if print_actions:\n",
    "                print(f\"action: {action_step.action}, reward: {time_step.reward}, return: {episode_return}\")\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7807545"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO include other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the replay buffer\n",
    "\n",
    "A replay buffer stores trajectories (sequences of states and actions) from data collection, and then samples those trajectories to train the agent. This increases data-efficiency and decreases bias.\n",
    "\n",
    "Trying to use the [EpisodicReplaybuffer](https://github.com/tensorflow/agents/blob/v0.5.0/tf_agents/replay_buffers/episodic_replay_buffer.py#L100) to return full episodes from the buffer. This is important when using a Q-RNN network, because the internal state must update starting from the beginning of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer at 0x7f8e8c84eef0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "#     data_spec=agent.collect_data_spec,\n",
    "#     batch_size=train_env.batch_size,\n",
    "#     max_length=replay_buffer_max_length)\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length,\n",
    ")\n",
    "\n",
    "replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    if time_step.is_last():\n",
    "        time_step = environment.reset()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "#     print(traj)\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect 64 episodes from a random policy and store to the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 16, 16, 4), dtype=float32, numpy=\n",
       "array([[[[ 9.99949932e-01, -1.00060096e-02,  9.99997914e-01,\n",
       "          -1.66666496e-03],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  4.16145253e-04,\n",
       "          -4.17186908e-04],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  4.16145253e-04,\n",
       "          -4.17186908e-04],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.44736126e-10,\n",
       "          -1.44615570e-10],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.44736126e-10,\n",
       "          -1.44615570e-10],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.20563227e-13,\n",
       "           1.66400347e-29]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00, -4.17186908e-04,\n",
       "          -4.16145253e-04],\n",
       "         [ 9.99995530e-01,  6.05578534e-04,  9.99998987e-01,\n",
       "          -8.33332771e-04],\n",
       "         [ 1.22075619e-06,  1.46589638e-03, -3.47221913e-07,\n",
       "           2.89351709e-10],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -5.04063186e-24,\n",
       "          -3.47222056e-07],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.78911188e-29,\n",
       "           1.20563227e-13],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.44615570e-10,\n",
       "          -1.44736126e-10]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00, -4.17186908e-04,\n",
       "          -4.16145253e-04],\n",
       "         [ 1.22075619e-06,  1.46589638e-03, -3.47221913e-07,\n",
       "           2.89351709e-10],\n",
       "         [ 9.99994278e-01, -2.99622235e-03,  9.99998987e-01,\n",
       "          -8.33332771e-04],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.42426371e-29,\n",
       "           1.20563227e-13],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  6.33310156e-24,\n",
       "          -3.47222056e-07],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.44615570e-10,\n",
       "          -1.44736126e-10]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  1.44615570e-10,\n",
       "          -1.44736126e-10],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -5.44646735e-23,\n",
       "           3.47222056e-07],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  2.39493240e-29,\n",
       "          -1.20563227e-13],\n",
       "         ...,\n",
       "         [ 9.99996781e-01,  2.00376566e-03,  9.99998987e-01,\n",
       "           8.33332771e-04],\n",
       "         [-6.10871029e-06,  1.46588415e-03, -3.47221913e-07,\n",
       "          -2.89351709e-10],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  4.17186908e-04,\n",
       "          -4.16145253e-04]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  1.44615570e-10,\n",
       "          -1.44736126e-10],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.21225734e-29,\n",
       "          -1.20563227e-13],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -4.91913971e-23,\n",
       "           3.47222056e-07],\n",
       "         ...,\n",
       "         [-6.10871029e-06,  1.46588415e-03, -3.47221913e-07,\n",
       "          -2.89351709e-10],\n",
       "         [ 9.99979973e-01,  5.60552767e-03,  9.99998987e-01,\n",
       "           8.33332771e-04],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  4.17186908e-04,\n",
       "          -4.16145253e-04]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00, -1.20563227e-13,\n",
       "          -3.10120943e-29],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.44736126e-10,\n",
       "          -1.44615570e-10],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.44736126e-10,\n",
       "          -1.44615570e-10],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -4.16145253e-04,\n",
       "          -4.17186908e-04],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -4.16145253e-04,\n",
       "          -4.17186908e-04],\n",
       "         [ 1.00000000e+00, -6.17695741e-06,  9.99997914e-01,\n",
       "           1.66666496e-03]]]], dtype=float32)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_env.reset()\n",
    "\n",
    "collect_data(env=train_env,\n",
    "    policy=random_policy, \n",
    "    buffer=replay_buffer,\n",
    "    steps=episode_length*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=TensorSpec(shape=(16, 16, 4), dtype=tf.float32, name=None), action=BoundedTensorSpec(shape=(), dtype=tf.int32, name=None, minimum=array(0, dtype=int32), maximum=array(4, dtype=int32)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensorflow `Dataset` takes care of sampling the replay buffer and generating trajectories quite nicely. The replay buffer can be converted to a `Dataset` which is then used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=2,\n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the driver\n",
    "\n",
    "TODO\n",
    "\n",
    "- see whether I actually need driver (seems slower...)\n",
    "- add writeup to this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "avg_return = tf_metrics.AverageReturnMetric()\n",
    "\n",
    "observers = [num_episodes,\n",
    "             avg_return,\n",
    "             replay_buffer.add_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env,\n",
    "    collect_policy,\n",
    "    observers,\n",
    "    num_steps=episode_length*1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time_step, policy_state = driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "agent.collect_policy.action = common.function(agent.collect_policy.action)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    train_env.reset()\n",
    "    policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "#         final_time_step, policy_state = driver.run()\n",
    "        for _ in range(collect_steps_per_iteration):\n",
    "            #print(policy_state)\n",
    "            collect_step(train_env,\n",
    "                         agent.collect_policy,\n",
    "                         replay_buffer)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience).loss\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            # print(q_net(np.zeros((1,5,5), dtype=\"float32\"))[0].numpy())\n",
    "            print(f'step = {step}: loss = {train_loss}')\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_env, agent.policy)\n",
    "            print(f'step = {step}: Average Return = {avg_return}')\n",
    "            if avg_return > 50:\n",
    "                break\n",
    "            returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f train_agent train_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the agent\n",
    "\n",
    "See what pulse sequences it's performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_avg_return(eval_env, agent.policy, num_episodes=1, print_actions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Q-network structure (including the encoding network, LSTM, and final dense layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_rnn_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = q_net.get_layer(\"EncodingNetwork\").get_weights()\n",
    "for weight in w:\n",
    "    print(weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what the Q-function returns for a play-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = train_env.reset()\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(1)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(2)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(4)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(3)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(0)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "print(ts.reward.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually interact with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.reset()\n",
    "# run the WHH-4 sequence\n",
    "eval_env.step(1)\n",
    "eval_env.step(2)\n",
    "eval_env.step(4)\n",
    "eval_env.step(3)\n",
    "eval_env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
