{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Sequence Design using PPO\n",
    "_Written by Will Kaufman_\n",
    "\n",
    "This notebook walks through a reinforcement learning approach to pulse sequence design for spin systems. [TF-Agents](https://www.tensorflow.org/agents) is used as a reinforcement learning library that uses Tensorflow, a common machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import spin_simulation as ss\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.environments import tf_py_environment, parallel_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network, value_network\n",
    "from tf_agents.policies import random_tf_policy, policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from environments import spin_sys_discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(spin_sys_discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define algorithm hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "episode_length = 5 # @param {type:\"integer\"}\n",
    "\n",
    "# collect parameters\n",
    "num_environment_steps = 5000  # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 20 # @param {type:\"integer\"}\n",
    "num_parallel_environments = 20 # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "#training parameters\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "\n",
    "# evaluation parameters\n",
    "num_eval_episodes = 5  # @param {type:\"integer\"}\n",
    "eval_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 12 #64  # @param {type:\"integer\"}\n",
    "\n",
    "# summaries and logging parameters\n",
    "train_checkpoint_interval=500\n",
    "policy_checkpoint_interval=500\n",
    "log_interval=50\n",
    "summary_interval=50\n",
    "summaries_flush_secs=1\n",
    "use_tf_functions=True\n",
    "debug_summaries=False\n",
    "summarize_grads_and_vars=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"~/projects/rl_pulse/data/\"\n",
    "\n",
    "root_dir = os.path.expanduser(root_dir)\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "eval_dir = os.path.join(root_dir, 'eval')\n",
    "saved_model_dir = os.path.join(root_dir, 'policy_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "train_summary_writer.set_as_default()\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the spin system\n",
    "\n",
    "This sets the parameters of the system ($N$ spin-1/2 particles, which corresponds to a Hilbert space with dimension $2^N$). For the purposes of simulation, $\\hbar \\equiv 1$.\n",
    "\n",
    "The total internal Hamiltonian is given by\n",
    "$$\n",
    "H_\\text{int} = C H_\\text{dip} + \\delta \\sum_i^N I_z^{i}\n",
    "$$\n",
    "where $C$ is the coupling strength, $\\delta$ is the chemical shift strength (each spin is assumed to be identical), and $H_\\text{dip}$ is given by\n",
    "$$\n",
    "H_\\text{dip} = \\sum_{i,j}^N d_{i,j} \\left(3I_z^{i}I_z^{j} - \\mathbf{I}^{i} \\cdot \\mathbf{I}^{j}\\right)\n",
    "$$\n",
    "\n",
    "The target Hamiltonian is set to be the 0th-order average Hamiltonian from the WHH-4 pulse sequence, which is designed to remove the dipolar interaction term from the internal Hamiltonian. The pulse sequence is $\\tau, \\overline{X}, \\tau, Y, \\tau, \\tau, \\overline{Y}, \\tau, X, \\tau$.\n",
    "The zeroth-order average Hamiltonian for the WAHUHA pulse sequence is\n",
    "$$\n",
    "H_\\text{WHH}^{(0)} = \\delta / 3 \\sum_i^N \\left( I_x^{i} + I_y^{i} + I_z^{i} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=4\n",
    "dim = 2**N\n",
    "coupling = 1e3\n",
    "delta = 500\n",
    "(X,Y,Z) = ss.get_total_spin(N=N, dim=dim)\n",
    "H_target = ss.get_H_WHH_0(X, Y, Z, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SpinSystemDiscreteEnv` class keeps track of the system dynamics, and implements methods that are necessary for RL:\n",
    "\n",
    "- `action_spec`: Returns an `ArraySpec` that gives the shape and range of a valid action. For example, in a discrete action space, an action will be an integer scalar between 0 and `numActions - 1`. For a continuous action space, an action will be a 3-dimensional vector representing phase, amplitude, and duration of the pulse.\n",
    "- `observation_spec`: Returns an `ArraySpec` that gives the shape and range of a valid observation. In this case, the observations are all the actions performed on the environment so far.\n",
    "- `_reset`: Resets the environment. This means setting the propagator to the identity, and choosing a new random dipolar interaction matrix $(d_{i,j})$.\n",
    "- `_step`: Evolves the environment according to the action. Returns a `TimeStep` which includes the step type (`FIRST`, `MID`, or `LAST`), the **reward**, the discount rate to apply to future rewards, and an **observation** of the environment.\n",
    "\n",
    "The reward function $r(s,a)$ can in general depend on the environment state _and_ action performed. However, because the goal of pulse sequence design is to find high-fidelity pulse sequences, the reward only depends on the state. \n",
    "$$\n",
    "r = -\\log \\left( 1-\n",
    "    \\left|\n",
    "        \\frac{\\text{Tr} (U_\\text{target}^\\dagger U_\\text{exp})}{\\text{Tr}(\\mathbb{1})}\n",
    "    \\right|\n",
    "    \\right)\n",
    "% = -\\log\\left( 1- \\text{fidelity}(U_\\text{target}, U_\\text{exp}) \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = spin_sys_discrete.SpinSystemDiscreteEnv(N=4, dim=16, coupling=1e3,\n",
    "    delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "    delay_after=True, state_size=episode_length)\n",
    "# env.reset()\n",
    "\n",
    "# train_py_env = spin_sys_discrete.SpinSystemDiscreteEnv(N=4, dim=16, coupling=1e3,\n",
    "#     delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "#     delay_after=True)\n",
    "# eval_py_env = spin_sys_discrete.SpinSystemDiscreteEnv(N=4, dim=16, coupling=1e3,\n",
    "#     delta=500, H_target=H_target, X=X, Y=Y, delay=5e-6, pulse_width=0,\n",
    "#     delay_after=True)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "\n",
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)\n",
    "\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define actor and value networks\n",
    "\n",
    "In PPO, there are two separate networks: the _actor_ network and the _value_ network. The actor network learns the policy function $\\pi(a|s)$, while the value network learns $v_\\pi(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params= (50, 50),\n",
    "    activation_fn=tf.keras.activations.tanh)\n",
    "\n",
    "value_net = value_network.ValueNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    fc_layer_params= (50, 50),\n",
    "    activation_fn=tf.keras.activations.tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the initial Q-values are for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net(train_env.current_time_step().observation)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net.get_layer(\"EncodingNetwork\").summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent\n",
    "\n",
    "In RL, the \"agent\" has a policy that determines its behavior. For DQN, the agent will act greedily during evaluation (i.e. it picks the action with the maximal Q-value) and epsilon-greedily during data collection. These policies are accessed with `agent.policy` (for evaluation) and `agent.collect_policy` (for data collection).\n",
    "\n",
    "According to [the docs](https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/tf_agent/TFAgent?hl=fa#args), I can adjust `train_sequence_length=None` for RNN-based agents. When using non-RNN DQN, though, I don't have that option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there a v2 optimizer I could use?\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\", dtype=tf.int64)\n",
    "\n",
    "agent = ppo_clip_agent.PPOClipAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    optimizer,\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    entropy_regularization=0.0,\n",
    "    importance_ratio_clipping=0.2,\n",
    "    normalize_observations=False,\n",
    "    normalize_rewards=False,\n",
    "    use_gae=True,\n",
    "    num_epochs=num_epochs,\n",
    "    debug_summaries=debug_summaries,\n",
    "    summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "    train_step_counter=global_step)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_steps_metric = tf_metrics.EnvironmentSteps()\n",
    "step_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    environment_steps_metric,\n",
    "]\n",
    "\n",
    "train_metrics = step_metrics + [\n",
    "    tf_metrics.AverageReturnMetric(\n",
    "        batch_size=1), # TODO replace with num_parallel_environments\n",
    "    tf_metrics.AverageEpisodeLengthMetric(\n",
    "        batch_size=1), # TODO replace with num_parallel_environments\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10, print_actions=False):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        policy_state = policy.get_initial_state(environment.batch_size)\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step, policy_state = policy_state)\n",
    "            policy_state = action_step.state\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            if print_actions:\n",
    "                print(f\"action: {action_step.action}, reward: {time_step.reward}, return: {episode_return}\")\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the replay buffer\n",
    "\n",
    "A replay buffer stores trajectories (sequences of states and actions) from data collection, and then samples those trajectories to train the agent. This increases data-efficiency and decreases bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length,\n",
    ")\n",
    "\n",
    "replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add checkpoints and policy saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=train_dir,\n",
    "    agent=agent,\n",
    "    global_step=global_step,\n",
    "    metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
    "policy_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(train_dir, 'policy'),\n",
    "    policy=eval_policy,\n",
    "    global_step=global_step)\n",
    "saved_model = policy_saver.PolicySaver(\n",
    "    eval_policy, train_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save some trajectories to the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    if time_step.is_last():\n",
    "        time_step = environment.reset()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    \n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect 64 episodes from a random policy and store to the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_step(train_env, collect_policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_env.reset()\n",
    "\n",
    "collect_data(env=train_env,\n",
    "    policy=collect_policy,\n",
    "    buffer=replay_buffer,\n",
    "    steps=episode_length*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensorflow `Dataset` takes care of sampling the replay buffer and generating trajectories quite nicely. The replay buffer can be converted to a `Dataset` which is then used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset generates trajectories with shape [Bx2x...]\n",
    "# dataset = replay_buffer.as_dataset(\n",
    "#     num_parallel_calls=2,\n",
    "#     sample_batch_size=batch_size, \n",
    "#     num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(dataset)\n",
    "\n",
    "# print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the driver\n",
    "\n",
    "TODO add writeup to this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    train_env,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch] + train_metrics,\n",
    "    num_episodes=collect_episodes_per_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    trajectories = replay_buffer.gather_all()\n",
    "    return agent.train(experience=trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert functions to `tf_function`s for speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_driver.run = common.function(collect_driver.run, autograph=False)\n",
    "agent.train = common.function(agent.train, autograph=False)\n",
    "train_step = common.function(train_step)\n",
    "#agent.collect_policy.action = common.function(agent.collect_policy.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "# avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "# returns = [avg_return]\n",
    "# print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_time = 0\n",
    "train_time = 0\n",
    "timed_at_step = global_step.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "# define some code\n",
    "#%lprun -f train_agent train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_agent():\n",
    "#     train_env.reset()\n",
    "#     policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "\n",
    "#     for _ in range(num_iterations):\n",
    "\n",
    "#         # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "# #         final_time_step, policy_state = driver.run()\n",
    "#         for _ in range(collect_steps_per_iteration):\n",
    "#             #print(policy_state)\n",
    "#             collect_step(train_env,\n",
    "#                          agent.collect_policy,\n",
    "#                          replay_buffer)\n",
    "\n",
    "#         # Sample a batch of data from the buffer and update the agent's network.\n",
    "#         experience, unused_info = next(iterator)\n",
    "#         train_loss = agent.train(experience).loss\n",
    "\n",
    "#         step = agent.train_step_counter.numpy()\n",
    "\n",
    "#         if step % log_interval == 0:\n",
    "#             # print(q_net(np.zeros((1,5,5), dtype=\"float32\"))[0].numpy())\n",
    "#             print(f'step = {step}: loss = {train_loss}')\n",
    "\n",
    "#         if step % eval_interval == 0:\n",
    "#             avg_return = compute_avg_return(eval_env, agent.policy)\n",
    "#             print(f'step = {step}: Average Return = {avg_return}')\n",
    "#             if avg_return > 50:\n",
    "#                 break\n",
    "#             returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Include eval [like this](https://github.com/tensorflow/agents/blob/v0.5.0/tf_agents/agents/ppo/examples/v2/train_eval_clip_agent.py#L238)\n",
    "- [ ] Continue debugging code below (lots of things I failed to define above...)\n",
    "- [ ] See what result is, if it works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while environment_steps_metric.result() < num_environment_steps:\n",
    "    global_step_val = global_step.numpy()\n",
    "    if global_step_val % eval_interval == 0:\n",
    "        metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    collect_driver.run()\n",
    "    collect_time += time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_loss, _ = train_step()\n",
    "    replay_buffer.clear()\n",
    "    train_time += time.time() - start_time\n",
    "    \n",
    "    for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=step_metrics)\n",
    "\n",
    "    if global_step_val % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step_val, total_loss)\n",
    "        steps_per_sec = (\n",
    "            (global_step_val - timed_at_step) / (collect_time + train_time))\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        logging.info('collect_time = %.3f, train_time = %.3f', collect_time,\n",
    "                     train_time)\n",
    "    with tf.compat.v2.summary.record_if(True):\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "\n",
    "    if global_step_val % train_checkpoint_interval == 0:\n",
    "        train_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "    if global_step_val % policy_checkpoint_interval == 0:\n",
    "        policy_checkpointer.save(global_step=global_step_val)\n",
    "        saved_model_path = os.path.join(\n",
    "            saved_model_dir, 'policy_' + ('%d' % global_step_val).zfill(9))\n",
    "        saved_model.save(saved_model_path)\n",
    "\n",
    "    timed_at_step = global_step_val\n",
    "    collect_time = 0\n",
    "    train_time = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the agent\n",
    "\n",
    "See what pulse sequences it's performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_avg_return(eval_env, agent.policy, num_episodes=1, print_actions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Q-network structure (including the encoding network, LSTM, and final dense layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_rnn_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = q_net.get_layer(\"EncodingNetwork\").get_weights()\n",
    "for weight in w:\n",
    "    print(weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what the Q-function returns for a play-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = train_env.reset()\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(1)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(2)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(4)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(3)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "ts = train_env.step(0)\n",
    "print(q_net(ts.observation, step_type=ts.step_type)[0].numpy())\n",
    "print(ts.reward.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually interact with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.reset()\n",
    "# run the WHH-4 sequence\n",
    "eval_env.step(1)\n",
    "eval_env.step(2)\n",
    "eval_env.step(4)\n",
    "eval_env.step(3)\n",
    "eval_env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
